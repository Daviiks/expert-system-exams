[
  {
    "id": 1,
    "question": "Сильный и слабый ИИ.",
    "answer": "Слабый ИИ (или узкий ИИ) — это то, что нас окружает сегодня: умные колонки, чат-боты, системы рекомендаций. Он умеет выполнять только одну конкретную задачу, для которой его запрограммировали. Например, шахматный движок может обыграть гроссмейстера, но не сможет поддержать разговор о погоде. Сильный ИИ (или общий ИИ) — это гипотетический интеллект, равный человеческому или превосходящий его. Он мог бы понимать, учиться и применять знания в абсолютно любой области, как человек, а не только в одной. Такого ИИ пока не существует, это скорее научно-фантастическая цель.",
    "topic": "Основные понятия ИИ"
  },
  {
    "id": 2,
    "question": "Эвристический алгоритм.",
    "answer": "Это «умная» стратегия поиска решения, которая работает быстро, но не всегда находит самый лучший вариант. Вместо того чтобы перебирать все возможные варианты (что может занять годы), алгоритм использует «правило большого пальца», логические догадки или прошлый опыт, чтобы быстро прийти к хорошему, пусть и не идеальному, решению. Например, таксист, выбирающий маршрут, чтобы объехать пробку, — использует эвристику. Он не проверяет все дороги города, но находит достаточно хороший путь.",
    "topic": "Алгоритмы"
  },
  {
    "id": 3,
    "question": "Прямая и обратная задачи.",
    "answer": "Прямая задача — это когда мы знаем причину и можем точно предсказать результат. Например, зная рецепт и количество ингредиентов (причина), мы можем вычислить, сколько пирога получится (следствие). Обратная задача — это когда мы видим результат и пытаемся понять, что его вызвало. Например, врач видит симптомы у пациента (следствие) и пытается поставить диагноз (причину). Обратные задачи часто сложнее, потому что одни и те же симптомы могут быть вызваны разными болезнями, и для их решения нужны дополнительные предположения или данные.",
    "topic": "Методология"
  },
  {
    "id": 4,
    "question": "Отличие машинного обучения и ИИ.",
    "answer": "Искусственный интеллект (ИИ) — это очень широкая область, цель которой — создание систем, способных выполнять задачи, требующие человеческого интеллекта (понимание языка, планирование, творчество). Машинное обучение (МО) — это не весь ИИ, а один из главных его инструментов. Если представить ИИ как цель «построить умный автомобиль», то МО — это конкретный двигатель для этого автомобиля. МО — это способ научить компьютер решать задачи, не программируя его явно под каждое правило, а показывая ему много примеров, чтобы он сам нашёл закономерности. Не все системы ИИ используют МО (есть и экспертные системы, и правила), но сегодня МО — основа большинства прорывов в ИИ.",
    "topic": "Основные понятия"
  },
  {
    "id": 5,
    "question": "Системный анализ (свойство системности).",
    "answer": "Системный анализ — это подход к изучению сложных вещей, который рассматривает их не как набор отдельных деталей, а как единое целое (систему), где все части связаны между собой и влияют друг на друга. Свойство системности означает, что целое (система) ведёт себя не так, как простая сумма её частей, а приобретает новые качества. Например, отдельно взятые процессор, память и монитор — это просто железки. Но соединённые вместе особым образом, они становятся компьютером — системой, которая умеет вычислять и показывать изображение. Системный анализ помогает понять эти взаимосвязи и правильно управлять сложными объектами.",
    "topic": "Методология"
  },
  {
    "id": 6,
    "question": "Учёные, причастные к появлению понятия ИИ.",
    "answer": "Идеи об искусственном мышлении были и раньше (ещё в мифах), но как научная дисциплина ИИ родился в 1950-е годы. Ключевые фигуры: 1) Алан Тьюринг — предложил тест для определения, может ли машина мыслить (тест Тьюринга, 1950). 2) Джон Маккарти — придумал сам термин «искусственный интеллект» и организовал знаменитую Дартмутскую конференцию в 1956 году, которая считается официальным днём рождения ИИ как науки. 3) Марвин Мински — один из организаторов той же конференции, крупнейший теоретик. 4) Клод Шеннон — создатель теории информации, интересовался шахматными программами. 5) Аллен Ньюэлл и Герберт Саймон — пионеры в создании программ, решающих логические задачи и моделирующих человеческое мышление.",
    "topic": "История ИИ"
  },
  {
    "id": 7,
    "question": "Причина появления понятия ИИ.",
    "answer": "Идея создания искусственного интеллекта возникла из нескольких источников. Во-первых, развитие вычислительной техники в 1940-50-х годах показало, что машины могут выполнять сложные вычисления. Во-вторых, учёные задумались: если мозг обрабатывает информацию, а компьютер тоже обрабатывает информацию — можно ли смоделировать мышление? Третья причина — практические задачи, такие как автоматизация сложных процессов, перевод языков, игра в шахматы. Также повлияла кибернетика — наука об управлении в живых организмах и машинах. В 1956 году группа учёных собралась на Дартмутскую конференцию, где решили, что пришло время создать новую науку об искусственном интеллекте.",
    "topic": "История ИИ"
  },
  {
    "id": 8,
    "question": "Задачи, решаемые ИИ.",
    "answer": "ИИ сегодня решает огромное количество задач. Основные категории: 1) Распознавание образов — лиц, голоса, изображений, текста. 2) Принятие решений — рекомендации в интернете, автономные автомобили, торговые роботы. 3) Обработка естественного языка — перевод, чат-боты, анализ тональности текста. 4) Прогнозирование — погода, спрос на товары, курсы акций. 5) Кластеризация и классификация — группировка клиентов, определение спама, диагностика болезней. 6) Генерация контента — создание текстов, изображений, музыки. 7) Управление — умные дома, промышленные роботы. 8) Игры — шахматы, го, видеоигры. Суть в том, что ИИ берёт на себя задачи, где нужно найти закономерности в больших данных или принять решение в условиях неопределённости.",
    "topic": "Основные понятия"
  },
  {
    "id": 9,
    "question": "Основные подходы, используемые в ИИ.",
    "answer": "В ИИ есть три основных подхода. 1) Символьный подход (классический ИИ) — использует логику и правила. Система работает с символами (понятиями) и применяет к ним правила вывода, как в математике. Пример — экспертные системы для диагностики. 2) Субсимвольный подход (машинное обучение) — не использует явные правила. Система учится на примерах, находя статистические закономерности в данных. Пример — нейронные сети для распознавания котиков на фото. 3) Гибридный подход — сочетает оба метода, чтобы использовать сильные стороны каждого. Например, система может извлекать правила из данных с помощью МО, а затем применять логику для рассуждений. Также есть бионический подход — прямое копирование принципов работы мозга (нейросети).",
    "topic": "Методология"
  },
  {
    "id": 10,
    "question": "Периоды в истории ИИ.",
    "answer": "История ИИ — это волны оптимизма и разочарования. 1) 1950-е — 1960-е: Зарождение и большой оптимизм ('весна ИИ'). Учёные верили, что через 20 лет будет создан ИИ уровня человека. Развивались логические программы, игры. 2) 1970-е — 1980-е: Первая 'зима ИИ' — финансирование сократилось, потому что ожидания не оправдались. Затем возрождение благодаря экспертным системам, которые принесли коммерческий успех. 3) 1990-е: Вторая 'зима' из-за ограничений экспертных систем. Но параллельно развивалось машинное обучение и нейросети. 4) 2000-е — настоящее время: 'Большая весна' благодаря большим данным, мощным компьютерам и прорывам в глубоком обучении. ИИ стал частью повседневной жизни.",
    "topic": "История ИИ"
  },
  {
    "id": 11,
    "question": "AI winter.",
    "answer": "'Зима ИИ' — это периоды в истории, когда интерес и финансирование исследований в области искусственного интеллекта резко падали. Это происходило из-за того, что завышенные ожидания не оправдывались. Первая зима (1970-е) наступила, когда стало ясно, что простые логические системы не могут решать реальные сложные задачи. Вторая зима (конец 1980-х — 1990-е) случилась, когда экспертные системы, популярные в бизнесе, оказались дорогими в обслуживании и неспособными учиться. Инвесторы и правительства разочаровывались и сокращали бюджеты. Эти периоды учили научное сообщество быть осторожнее с прогнозами и показали, что развитие ИИ — это долгий путь с препятствиями.",
    "topic": "История ИИ"
  },
  {
    "id": 12,
    "question": "Парадокс Моравека.",
    "answer": "Это наблюдение исследователей в области ИИ о том, что для компьютера сложные интеллектуальные задачи (например, игра в шахматы или доказательство теорем) оказываются проще, чем базовые сенсомоторные навыки, которые даются младенцам и животным (например, ходьба, распознавание предметов, понимание речи в шумной комнате). Парадокс в том, что то, что человеку кажется лёгким и не требующим размышлений (интуитивное), для ИИ — невероятно сложно. И наоборот: то, что для человека требует усилий и обучения (высшая математика), для ИИ может быть простым перебором или вычислением. Причина в том, что сенсоморные навыки — результат миллионов лет эволюции, отточенные до автоматизма, а абстрактное мышление — недавнее приобретение человечества.",
    "topic": "Философия ИИ"
  },
  {
    "id": 13,
    "question": "Проклятие размерности и комбинаторный взрыв.",
    "answer": "Это две большие проблемы в ИИ и анализе данных. 'Проклятие размерности' — когда данных слишком много признаков (измерений). Например, если у объекта 1 признак (длина), его легко анализировать. Если признаков 100 (длина, цвет, форма, текстура и т.д.), данных для надёжного анализа нужно экспоненциально больше, а расстояния между точками становятся бессмысленными, всё 'размазывается'. 'Комбинаторный взрыв' — это когда число возможных вариантов растёт не линейно, а взрывно (экспоненциально). Например, в шахматах число возможных партий больше, чем атомов во Вселенной. Полный перебор всех вариантов невозможен даже для суперкомпьютера. Именно поэтому нужны эвристики и машинное обучение — чтобы находить решения, не перебирая всё.",
    "topic": "Теория"
  },
  {
    "id": 14,
    "question": "Компьютеры пятого поколения.",
    "answer": "Это амбициозный японский проект 1980-х годов. Цель была — создать не просто новые быстрые компьютеры, а компьютеры, принципиально работающие иначе. Вместо традиционной архитектуры (процессор + память + программа) планировались машины для параллельной обработки логических правил и символьных вычислений — то есть идеально подходящие для задач ИИ, особенно для экспертных систем и логического программирования (язык Prolog). Проект потратил сотни миллионов долларов, но не достиг всех целей. Коммерчески он не победил, потому что параллельно развивались персональные компьютеры и стандартные микропроцессоры, которые оказались более универсальными и дешёвыми. Однако проект стимулировал исследования в параллельных вычислениях и логическом программировании.",
    "topic": "История ИИ"
  },
  {
    "id": 15,
    "question": "Семантический веб.",
    "answer": "Семантический веб — это идея Тим Бернерса-Ли (создателя WWW) о следующем поколении Интернета, где информация будет не только читаться людьми, но и пониматься машинами. Сейчас веб-страницы — это текст и картинки для людей. Поисковик может найти слова, но не понимает их смысл. В семантическом вебе данные размечаются специальными тегами и связями (например, 'этот текст — автор этой книги', 'этот объект — цена товара'), образуя гигантскую базу знаний. Тогда программа (агент) сможет сама сделать вывод: 'найти мне книги по ИИ, написанные после 2010 года, и показать рецензии на них'. Это позволит создавать умные сервисы, которые действительно понимают запросы. Это шаг к более интеллектуальному взаимодействию человека с информацией.",
    "topic": "Технологии"
  },
  {
    "id": 16,
    "question": "Технологическая сингулярность.",
    "answer": "Это гипотетический момент в будущем, когда развитие технологий (особенно ИИ) станет настолько быстрым и неконтролируемым, что его невозможно будет предсказать или понять с сегодняшней точки зрения. Идея в том, что если создать ИИ, который умнее человека, он сможет сам создавать ещё более умный ИИ, и этот процесс ускорится лавинообразно. После сингулярности мир может измениться до неузнаваемости — исчезнут болезни, старение, или, по пессимистичным сценариям, человечество утратит контроль. Это спорная концепция, больше философская и футуристическая, чем научная. Многие учёные сомневаются в её реалистичности, но она активно обсуждается в контексте этики и безопасности сильного ИИ.",
    "topic": "Философия ИИ"
  },
  {
    "id": 17,
    "question": "Тест Тьюринга, китайская комната и солипсизм.",
    "answer": "Это три концепции, связанные с вопросом 'может ли машина мыслить?'. 1) Тест Тьюринга: если человек, общаясь вслепую с компьютером и другим человеком, не может отличить компьютер от человека, то компьютер можно считать разумным. Это поведенческий тест — не важно 'как' внутри, важно 'как' снаружи. 2) Мысленный эксперимент 'Китайская комната' (Джон Сёрл) критикует этот тест. Человек в комнате, не знающий китайского, но имеющий инструкции по манипуляции иероглифами, может давать осмысленные ответы, не понимая языка. Так и компьютер может симулировать понимание, не имея сознания. 3) Солипсизм в этом контексте — крайняя позиция: мы никогда не можем быть уверены, что у кого-то кроме нас есть сознание. Так и с ИИ — даже если он пройдёт тест Тьюринга, мы не сможем доказать, что он 'понимает' что-то, а не просто вычисляет.",
    "topic": "Философия ИИ"
  },
  {
    "id": 18,
    "question": "Этические вопросы создания и существования ИИ.",
    "answer": "Развитие ИИ поднимает множество этических проблем. 1) Безопасность: как сделать ИИ предсказуемым и надёжным, чтобы он не причинил вреда по ошибке? 2) Контроль: кто отвечает за действия автономного ИИ (робота-водителя, врачебного ассистента)? 3) Смещение (bias): ИИ обучается на данных людей, поэтому может унаследовать и усилить наши предрассудки (расовые, гендерные). 4) Конфиденциальность: ИИ для распознавания лиц и анализа поведения угрожает приватности. 5) Влияние на работу: автоматизация лишит работы миллионы людей — как общество к этому подготовить? 6) Военное применение: автономное оружие, принимающее решение убивать. 7) Долгосрочные риски: что, если сильный ИИ превзойдёт человека и будет иметь свои цели? Эти вопросы требуют совместной работы технологов, философов, юристов и политиков.",
    "topic": "Этика ИИ"
  },
  {
    "id": 19,
    "question": "Дилемма вагонетки.",
    "answer": "Это классический этический мысленный эксперимент: неуправляемая вагонетка несётся по рельсам. На её пути привязаны пять человек. Вы стоите у стрелки и можете перевести вагонетку на запасной путь, где привязан один человек. Что делать? Убить одного, чтобы спасти пятерых? Этот эксперимент стал популярен в контексте этики автономных автомобилей. Как запрограммировать автомобиль в безвыходной ситуации: врезаться в группу пешеходов или резко свернуть, рискуя жизнью пассажиров? Проблема в том, что не существует однозначно 'правильного' морального ответа, но инженерам придётся делать этот выбор при программировании. Дилемма показывает, что разработка ИИ — это не только техника, но и глубокие этические решения.",
    "topic": "Этика ИИ"
  },
  {
    "id": 20,
    "question": "Антропоморфность ИИ (зловещая долина).",
    "answer": "Антропоморфность — это придание ИИ или роботу человеческих черт (внешности, голоса, манер), чтобы людям было комфортнее с ним взаимодействовать. Однако здесь кроется ловушка — 'зловещая долина'. Это эффект, когда робот или CGI-персонаж выглядит почти как человек, но небольшие несоответствия (стеклянный взгляд, неестественная мимика) вызывают у людей сильное отвращение и чувство тревоги. Кажется, что это 'оживший труп' или 'зомби'. Поэтому дизайнеры часто сознательно делают роботов не полностью похожими на людей (как R2-D2 или Валл-И), либо, наоборот, стремятся к фотореализму, чтобы преодолеть эту долину. Этот эффект важно учитывать при создании интерфейсов, чат-ботов и социальных роботов.",
    "topic": "Взаимодействие человек-ИИ"
  },
  {
    "id": 21,
    "question": "Случайное событие.",
    "answer": "Случайное событие — это такое событие, которое в результате какого-либо опыта (эксперимента, наблюдения) может произойти или не произойти, и заранее точно предсказать это невозможно. Например, выпадение 'орла' при подбрасывании монеты, дождь завтра, поломка детали в машине. Важно, что у случайности есть своя закономерность — вероятность. Хотя одно подбрасывание монеты непредсказуемо, при тысяче подбрасываний 'орёл' выпадет примерно в половине случаев. В ИИ и анализе данных мы часто имеем дело со случайными событиями — кликнет ли пользователь по рекламе, будет ли транзакция мошеннической, — и пытаемся оценить их вероятность на основе данных.",
    "topic": "Теория вероятностей"
  },
  {
    "id": 22,
    "question": "Вероятность и частота.",
    "answer": "Это два связанных, но разных понятия. Частота события — это конкретное число, которое мы получаем после эксперимента. Например, мы подбросили монету 100 раз, и 'орёл' выпал 47 раз. Частота выпадения 'орла' = 47/100 = 0.47. Вероятность же — это теоретическая, идеальная величина, которая показывает, с какой долей событие должно происходить в бесконечно длинной серии испытаний. Для честной монеты вероятность 'орла' = 0.5. Частота приближается к вероятности, когда число испытаний очень велико (закон больших чисел). В машинном обучении мы часто не знаем истинную вероятность, поэтому используем частоту, вычисленную на обучающих данных, как её оценку.",
    "topic": "Теория вероятностей"
  },
  {
    "id": 23,
    "question": "Суммарная вероятность.",
    "answer": "Суммарная вероятность (обычно имеется в виду полная вероятность) — это способ найти вероятность сложного события, разбив его на несколько более простых, непересекающихся случаев. Представьте, что есть две фабрики, которые производят лампочки. Первая делает 60% всех лампочек, вторая — 40%. У первой 2% брака, у второй — 5%. Какова вероятность, что случайно купленная лампочка окажется бракованной? Мы не знаем, с какой фабрики она. Тогда полная вероятность = (вероятность взять лампочку с 1-й фабрики) * (вероятность брака на 1-й) + (вероятность взять со 2-й) * (вероятность брака на 2-й) = 0.6*0.02 + 0.4*0.05 = 0.032. Это и есть общая вероятность брака. Формула помогает учесть все возможные пути к интересующему нас событию.",
    "topic": "Теория вероятностей"
  },
  {
    "id": 24,
    "question": "Зависимые и независимые случайные события.",
    "answer": "Два события независимы, если наступление одного из них не меняет вероятность наступления другого. Классический пример: два подбрасывания одной монеты. Выпал 'орёл' в первый раз — это никак не влияет на то, что выпадет во второй. Их вероятности независимы. События зависимы, если вероятность одного меняется при наступлении другого. Пример: вероятность того, что на улице дождь (событие А), и вероятность того, что человек взял зонт (событие В). Если мы знаем, что на улице дождь, то вероятность того, что у человека есть зонт, возрастает. В машинном обучении упрощающее предположение о независимости признаков (наивный байесовский классификатор) часто используется, чтобы упростить вычисления, хотя в реальности признаки часто зависимы.",
    "topic": "Теория вероятностей"
  },
  {
    "id": 25,
    "question": "Совместные и несовместные случайные события. Полная группа несовместных случайных событий.",
    "answer": "События несовместны, если они не могут произойти одновременно в одном эксперименте. Например, при броске кубика события 'выпало 1' и 'выпало 6' несовместны. События совместны, если они могут произойти вместе. Например, 'идёт дождь' и 'человек в пальто' — могут быть одновременно. Полная группа несовместных событий — это такой набор событий, который охватывает ВСЕ возможные исходы эксперимента, и при этом любые два события из этого набора несовместны. Например, для кубика события 'выпало 1', 'выпало 2', ..., 'выпало 6' образуют полную группу. Сумма их вероятностей равна 1. Это важное понятие для разбиения пространства исходов при вычислении вероятностей.",
    "topic": "Теория вероятностей"
  },
  {
    "id": 26,
    "question": "Случайные величины и способы их описания.",
    "answer": "Случайная величина — это не конкретное число, а переменная, которая в результате эксперимента принимает какое-то числовое значение случайным образом. Например, рост случайно выбранного человека, количество осадков за день, время безотказной работы прибора. Описать случайную величину можно несколькими способами: 1) Закон распределения (таблица, график) — для дискретных величин (как кубик). 2) Функция распределения F(x) — вероятность того, что величина примет значение меньше x. 3) Плотность распределения f(x) — для непрерывных величин (как рост). Производная от функции распределения. 4) Числовые характеристики: математическое ожидание (среднее значение), дисперсия (разброс вокруг среднего), мода, медиана. В анализе данных мы часто работаем с выборками из случайных величин, пытаясь оценить их свойства.",
    "topic": "Теория вероятностей"
  },
  {
    "id": 27,
    "question": "Центральные и начальные моменты случайных величин.",
    "answer": "Моменты — это числовые характеристики, которые подробно описывают форму распределения случайной величины. Начальный момент k-го порядка — это математическое ожидание k-й степени случайной величины: E[X^k]. Самый известный начальный момент первого порядка — это просто среднее значение (математическое ожидание). Центральный момент k-го порядка — это математическое ожидание k-й степени отклонения величины от её среднего: E[(X - E[X])^k]. Самый важный центральный момент второго порядка — это дисперсия (мера разброса). Центральный момент третьего порядка, нормированный, называется коэффициентом асимметрии (показывает, насколько распределение несимметрично). Центральный момент четвёртого порядка, нормированный, — это эксцесс (показывает 'остроту' или 'плоскость' пика распределения по сравнению с нормальным).",
    "topic": "Теория вероятностей"
  },
  {
    "id": 28,
    "question": "Равномерное и нормальное распределение.",
    "answer": "Это два фундаментальных типа распределений. 1) Равномерное распределение: все значения на некотором интервале [a, b] равновероятны. Например, вращение стрелки рулетки, ошибка округления. Плотность распределения — постоянная прямая линия. 2) Нормальное распределение (распределение Гаусса, 'колокол') — самое важное в статистике. Большинство значений сконцентрировано вокруг среднего, и по мере удаления в обе стороны вероятность убывает по экспоненциальному закону. Его форма задаётся всего двумя параметрами: средним (μ, центр колокола) и стандартным отклонением (σ, ширина колокола). Оно часто возникает в природе (рост, ошибки измерений) благодаря центральной предельной теореме: сумма большого числа мелких независимых случайных величин имеет распределение, близкое к нормальному. Это основа многих статистических методов и допущений в машинном обучении.",
    "topic": "Теория вероятностей"
  },
  {
    "id": 29,
    "question": "Априорная и апостериорная вероятности.",
    "answer": "Эти понятия происходят из байесовской теории вероятностей. Априорная вероятность ('доопытная') — это наша первоначальная оценка вероятности гипотезы ДО того, как мы увидели новые данные. Она основана на предыдущем знании, вере или просто предположении. Например, априорная вероятность того, что пациент болен редкой болезнью, может быть 0.1% (частота болезни в популяции). Апостериорная вероятность ('послеопытная') — это обновлённая вероятность гипотезы ПОСЛЕ того, как мы учли новые данные (доказательства). Например, после того как у пациента проявился специфический симптом, вероятность болезни пересчитывается (часто возрастает). Переход от априорной к апостериорной вероятности происходит по формуле Байеса. Это сердце байесовского подхода в машинном обучении и экспертных системах — постоянное обновление убеждений по мере поступления данных.",
    "topic": "Теория вероятностей"
  },
  {
    "id": 30,
    "question": "Экспертные системы.",
    "answer": "Экспертные системы (ЭС) — это одна из первых успешных форм ИИ, популярная в 1980-х. Их цель — копировать знания и логику рассуждений эксперта-человека в узкой предметной области (например, диагностика болезней, конфигурация компьютеров, анализ геологических данных). Состоят из трёх основных частей: 1) База знаний — набор фактов и правил типа 'ЕСЛИ температура > 38 И есть кашель, ТО возможен грипп'. 2) Механизм логического вывода — программа, которая применяет правила к конкретным фактам о ситуации, чтобы прийти к заключению. 3) Интерфейс пользователя — чтобы задавать вопросы и получать объяснения. Главное отличие от современных нейросетей в том, что экспертные системы прозрачны: можно посмотреть, по какому правилу было принято решение, и объяснить его пользователю.",
    "topic": "Экспертные системы"
  },
  {
    "id": 31,
    "question": "Байесовская экспертная система.",
    "answer": "Это особый тип экспертной системы, где знания представлены в вероятностной форме, а механизм вывода использует теорему Байеса для обновления вероятностей гипотез. Вместо жёстких правил 'ЕСЛИ-ТО' здесь есть вероятностные связи между симптомами (признаками, данными) и диагнозами (гипотезами). Например, мы знаем априорную вероятность болезни P(Болезнь) и условные вероятности симптомов при этой болезни P(Симптом|Болезнь). Когда пациент сообщает о симптомах, система вычисляет апостериорную вероятность каждой возможной болезни P(Болезнь|Симптомы) и предлагает наиболее вероятную. Такие системы хорошо справляются с неопределённостью и шумными данными. Классический пример — система диагностики в медицине или неисправностей в технике. Их слабое место — необходимость знать все вероятности заранее, что часто сложно.",
    "topic": "Экспертные системы"
  },
  {
    "id": 32,
    "question": "Неявные ограничения, накладываемые на экспертные системы с байесовским выводом.",
    "answer": "У байесовских экспертных систем есть несколько серьёзных ограничений, которые не всегда очевидны с первого взгляда. 1) Предположение о независимости признаков: для простоты вычислений часто предполагается, что симптомы или признаки условно независимы при данной гипотезе. В реальности симптомы болезни часто связаны (например, кашель и боль в горле), что нарушает это предположение и искажает результат. 2) Требование полноты данных: нужно знать ВСЕ априорные вероятности болезней и ВСЕ условные вероятности симптомов для каждой болезни. На практике такие полные и точные статистические данные получить очень трудно. 3) Дискретность: классический байесовский вывод плохо работает с непрерывными признаками без их дискретизации, что ведёт к потере информации. 4) Сложность масштабирования: при большом числе гипотез и признаков вычисления становятся слишком громоздкими.",
    "topic": "Экспертные системы"
  },
  {
    "id": 33,
    "question": "Классификация экспертных систем по решаемым задачам.",
    "answer": "Экспертные системы можно разделить на типы в зависимости от того, какую задачу они решают: 1) Интерпретация: анализ данных для описания ситуации (например, анализ сейсмограмм для определения структуры грунта). 2) Диагностика: выявление причин неисправностей в системах (медицина, техника). 3) Мониторинг: непрерывное сравнение данных с нормальными показателями и сигнализация об отклонениях (контроль на АЭС, наблюдение за пациентом). 4) Прогнозирование: предсказание будущих событий на основе моделей (прогноз погоды, урожая). 5) Планирование: составление последовательности действий для достижения цели (планирование военных операций, расписаний). 6) Проектирование: конфигурирование объектов по заданным требованиям (размещение микросхем на плате, архитектурное проектирование). 7) Обучение: диагностика ошибок ученика и подбор упражнений (обучающие системы). У каждой задачи свои особенности представления знаний и механизма вывода.",
    "topic": "Экспертные системы"
  },
  {
    "id": 34,
    "question": "Классификация ЭС по внутреннему устройству.",
    "answer": "По тому, как устроены база знаний и механизм вывода, экспертные системы делятся на: 1) Системы, основанные на правилах (продукционные системы). Самая распространённая архитектура. Знания хранятся в виде набора правил 'ЕСЛИ условие, ТО действие'. Механизм вывода сопоставляет факты с условиями правил и активирует их. Может работать в прямом направлении (от данных к цели) или обратном (от гипотезы к данным). 2) Системы, основанные на фреймах. Знания представлены в виде фреймов — структур данных, описывающих типовые объекты или ситуации (например, фрейм 'студент' с полями 'имя', 'курс', 'специальность'). Вывод идёт через сопоставление и наследование свойств. 3) Системы, основанные на семантических сетях. Знания — это граф, где узлы — понятия, а дуги — отношения между ними (часть, вид, причина). Вывод осуществляется путём распространения по сети. 4) Системы, основанные на логике (исчисление предикатов). Знания — логические утверждения. Механизм вывода — логический вывод, резолюция. 5) Гибридные системы, сочетающие несколько подходов.",
    "topic": "Экспертные системы"
  },
  {
    "id": 35,
    "question": "Формулы байесовского вывода.",
    "answer": "Основная формула — это теорема Байеса, которая связывает априорную и апостериорную вероятности: P(H|E) = [P(E|H) * P(H)] / P(E). Где: P(H) — априорная вероятность гипотезы H (насколько мы верили в неё до данных). P(E|H) — правдоподобие: вероятность наблюдать доказательства E, если гипотеза H верна. P(E) — полная вероятность доказательств (нормировочная константа). P(H|E) — апостериорная вероятность гипотезы H после учёта доказательств E. P(E) можно вычислить по формуле полной вероятности: P(E) = Σ [P(E|H_i) * P(H_i)] по всем возможным гипотезам H_i. На практике часто работают с отношениями шансов (odds), чтобы избежать вычисления P(E). Формула обновления шансов: Апостериорные шансы = Априорные шансы * Отношение правдоподобия.",
    "topic": "Экспертные системы / Теория вероятностей"
  },
  {
    "id": 36,
    "question": "Вычисление апостериорной вероятности при неоднозначных ответах пользователя (скорее да, 8 из 10, и.т.п.).",
    "answer": "В реальных экспертных системах пользователь не всегда отвечает 'да' или 'нет' точно. Он может сказать 'скорее да', 'возможно', 'на 80% уверен'. Для работы с такими нечёткими ответами в байесовских системах вводят понятие 'веса свидетельства' или коэффициента уверенности (CF — certainty factor). Вместо двоичного значения симптома (есть/нет) используется вероятность или степень уверенности в наличии симптома, например, p = 0.8. Тогда правдоподобие P(E|H) в формуле Байеса модифицируется. Один из подходов: использовать линейную интерполяцию. Если для точного наличия симптома правдоподобие = P(E=да|H), а для точного отсутствия = P(E=нет|H), то для уверенности p правдоподобие вычисляется как: p * P(E=да|H) + (1-p) * P(E=нет|H). Это позволяет плавно обновлять вероятности гипотез в соответствии с нечёткой информацией от пользователя.",
    "topic": "Экспертные системы"
  },
  {
    "id": 37,
    "question": "Анализ данных.",
    "answer": "Анализ данных — это процесс исследования, очистки, преобразования и моделирования данных с целью извлечения полезной информации, принятия решений и обнаружения скрытых закономерностей. Это не просто статистика, а целый цикл: от постановки бизнес-вопроса до внедрения решения. Основные этапы: 1) Сбор и понимание данных. 2) Предобработка (очистка от ошибок, заполнение пропусков). 3) Исследовательский анализ (визуализация, выявление аномалий, гипотезы). 4) Моделирование (применение алгоритмов машинного обучения или статистики). 5) Интерпретация результатов и выводы. Анализ данных используется везде: в бизнесе (анализ клиентов, прогноз продаж), науке (обработка экспериментов), медицине, социологии, IT (анализ логов). Главная цель — превратить сырые данные в знания, на которые можно опереться при принятии решений.",
    "topic": "Анализ данных"
  },
  {
    "id": 38,
    "question": "Big data.",
    "answer": "Big Data ('большие данные') — это не просто много данных. Это такие объёмы, разнообразие и скорость поступления информации, которые невозможно эффективно обрабатывать традиционными методами и базами данных. Характеризуются тремя (или более) V: 1) Volume (объём) — терабайты и петабайты. 2) Velocity (скорость) — данные поступают и обновляются с огромной скоростью (соцсети, датчики). 3) Variety (разнообразие) — данные разного формата: структурированные (таблицы), полуструктурированные (XML, JSON), неструктурированные (тексты, фото, видео). Часто добавляют Veracity (достоверность) — данные могут быть зашумлёнными, и Value (ценность) — извлечение пользы. Для работы с Big Data нужны специальные технологии: распределённые файловые системы (Hadoop HDFS), фреймворки для параллельной обработки (MapReduce, Spark), NoSQL базы данных (MongoDB, Cassandra). Именно Big Data стали 'топливом' для современного машинного обучения.",
    "topic": "Анализ данных"
  },
  {
    "id": 39,
    "question": "Map-reduce.",
    "answer": "MapReduce — это модель программирования и фреймворк от Google для параллельной обработки огромных массивов данных на кластере из тысяч компьютеров. Идея в разбиении задачи на две простые фазы. 1) Фаза Map (отображение): Каждый узел кластера обрабатывает свой небольшой кусок входных данных и выдаёт набор промежуточных пар 'ключ-значение'. Например, для подсчёта частоты слов: Map читает текст и для каждого слова выдаёт пару (слово, 1). 2) Фаза Reduce (свёртка): Данные группируются по ключам (все пары с одинаковым ключом собираются на одном узле), и для каждой группы выполняется агрегация. В примере со словами: Reduce получает все пары (слово, [1,1,1...]) и суммирует единицы, получая итоговое количество. Пользователь пишет только функции Map и Reduce, а система сама распределяет данные, обрабатывает сбои и координирует работу. Это позволило обрабатывать петабайты данных на дешёвом 'обычном' железе. Apache Hadoop — популярная реализация этой концепции.",
    "topic": "Анализ данных / Big Data"
  },
  {
    "id": 40,
    "question": "Типы измерительных шкал.",
    "answer": "Измерительные шкалы определяют, как мы можем работать с данными, какие операции и статистики к ним применимы. Есть четыре основных типа, от самых слабых к сильным: 1) Номинальная (категориальная) — данные — это просто метки или имена, без порядка. Пример: цвет глаз (синий, карий), пол, код города. Можно только проверять равенство. Статистики: мода, частота. 2) Порядковая (ординальная) — данные можно упорядочить по рангу, но разница между значениями не определена. Пример: оценки (отлично, хорошо, удовлетворительно), уровень боли (легкая, средняя, сильная). Можно сравнивать 'больше-меньше'. Статистики: медиана, процентили. 3) Интервальная — есть порядок и определённые расстояния между значениями, но нет истинного нуля. Пример: температура по Цельсию (0°C — не отсутствие температуры). Можно складывать и вычитать. Статистики: среднее, стандартное отклонение. 4) Шкала отношений — есть и порядок, и расстояния, и истинный ноль. Пример: рост, вес, возраст, количество денег. Можно умножать и делить ('в два раза тяжелее'). Применяются все статистики. Важно правильно определить шкалу, чтобы выбрать корректный метод анализа.",
    "topic": "Анализ данных"
  },
  {
    "id": 41,
    "question": "Куб данных.",
    "answer": "Куб данных (Data Cube) — это многомерная структура данных, которая позволяет анализировать информацию по нескольким измерениям одновременно. Представьте обычную таблицу Excel с продажами: строки — товары, столбцы — месяцы. Это двумерный вид. Теперь добавьте третье измерение — регионы. Получится куб, где по оси X — время, по оси Y — товары, по оси Z — регионы. В ячейках на пересечении — объём продаж. Куб позволяет легко делать срезы (slice) — смотреть на данные по одному измерению (например, все продажи в Москве) и свертки (dice/roll-up) — агрегировать данные (например, сумма продаж по всем регионам за год). Это основа OLAP-систем (Online Analytical Processing), которые используются для бизнес-анализа. Куб помогает быстро отвечать на вопросы вроде «Какой товар лучше всего продавался в Сибири во втором квартале?» без сложных запросов к базам данных.",
    "topic": "Анализ данных"
  },
  {
    "id": 42,
    "question": "Три типа задач анализа данных.",
    "answer": "Задачи анализа данных обычно делят на три большие категории по целям: 1) Описательные (Descriptive) — ответ на вопрос «Что произошло?». Это констатация фактов: суммарные продажи, количество пользователей, средний чек. Сюда входят отчётность, дашборды, описательная статистика. 2) Диагностические (Diagnostic) — ответ на вопрос «Почему это произошло?». Поиск причин и взаимосвязей: почему упали продажи в марте? Почему вырос отток клиентов? Используются методы корреляционного анализа, детализации данных (drill-down). 3) Предсказательные (Predictive) — ответ на вопрос «Что произойдёт?». Прогнозирование будущих событий на основе исторических данных: какой будет спрос, кто из клиентов уйдёт, будет ли транзакция мошеннической. Это область машинного обучения (регрессия, классификация). Есть ещё четвёртый тип — предписывающие (Prescriptive) — «Что нужно сделать?» — рекомендации действий для достижения цели (оптимизация).",
    "topic": "Анализ данных"
  },
  {
    "id": 43,
    "question": "Очистка данных.",
    "answer": "Очистка данных (Data Cleaning) — это, пожалуй, самый важный и трудоёмкий этап в анализе данных, на который может уходить до 80% времени. Это процесс обнаружения и исправления (или удаления) ошибок и несоответствий в данных для повышения их качества. Что обычно чистят: 1) Пропущенные значения (NaN, NULL). 2) Выбросы (аномальные значения, которые сильно отличаются от остальных — например, возраст 200 лет). 3) Несоответствия форматов (даты в разном виде, «муж»/«М»/«male» для одного признака). 4) Дубликаты (одинаковые строки). 5) Шум (случайные ошибки в данных, опечатки). 6) Некорректные зависимости (например, ребёнок старше родителя). Методы очистки: удаление строк, заполнение средним/медианой/модой, интерполяция, использование алгоритмов для предсказания пропущенных значений. Без качественной очистки даже самые сложные модели будут выдавать бессмысленные результаты.",
    "topic": "Анализ данных"
  },
  {
    "id": 44,
    "question": "Заполнение пробелов.",
    "answer": "Заполнение пробелов (Imputation) — это подкатегория очистки данных, посвящённая методам работы с пропущенными значениями (пробелами, NaN). Просто удалить строки с пропусками часто нельзя — потеряется слишком много информации. Поэтому пробелы заполняют. Основные методы: 1) Заполнение константой (например, 0 или «Неизвестно»). 2) Заполнение статистическими показателями: для числовых признаков — средним, медианой; для категориальных — модой (самым частым значением). 3) Заполнение с использованием взаимосвязей: например, если пропущен возраст, а есть год рождения, можно его вычислить. 4) Более сложные методы: использование моделей машинного обучения (k-NN, регрессия) для предсказания пропущенных значений на основе других признаков. Например, по зарплате, образованию и должности предсказать пропущенный возраст. Важно понимать причину пропусков: пропущены ли они случайно или систематически (например, богатые люди чаще отказываются указывать доход) — от этого зависит стратегия заполнения.",
    "topic": "Анализ данных"
  },
  {
    "id": 45,
    "question": "Выявление информативных признаков.",
    "answer": "Выявление информативных признаков (Feature Selection) — это процесс отбора тех переменных (признаков, полей) в данных, которые действительно полезны для построения модели, и отсева бесполезных или вредных (шумовых). Зачем это нужно: 1) Улучшение качества модели — лишние признаки могут приводить к переобучению. 2) Ускорение обучения модели — меньше данных для обработки. 3) Упрощение модели и её интерпретации. Методы делятся на: 1) Фильтровые: оценивают полезность признака независимо от модели, на основе статистических критериев (например, корреляция с целевой переменной, ANOVA, χ²). 2) Встроенные (Embedded): отбор происходит в процессе обучения модели (например, L1-регуляризация в линейных моделях зануляет неважные коэффициенты). 3) Методы-обёртки (Wrapper): перебирают подмножества признаков, обучая модель на каждом и оценивая её качество (жадные алгоритмы, генетические алгоритмы). Это ресурсоёмко, но эффективно. Хороший отбор признаков часто важнее выбора сложного алгоритма.",
    "topic": "Анализ данных / Машинное обучение"
  },
  {
    "id": 46,
    "question": "Линейная регрессия.",
    "answer": "Линейная регрессия — это один из самых простых и распространённых методов машинного обучения для решения задач регрессии (предсказания числового значения). Идея: найти такую прямую линию (или гиперплоскость в многомерном случае), которая лучше всего описывает зависимость между целевой переменной (y) и одним или несколькими признаками (x). «Лучше всего» означает минимизацию суммы квадратов ошибок (разниц между предсказанным значением и реальным) — метод наименьших квадратов. Формула для одного признака: y = w1*x + w0, где w1 — наклон (коэффициент), w0 — смещение (интерцепт). Алгоритм находит оптимальные w1 и w0. Плюсы: простота, интерпретируемость (можно сказать: «при увеличении X на 1, Y увеличивается на w1»), быстродействие. Минусы: предполагает линейную зависимость, что редко бывает в реальности; чувствительна к выбросам. Тем не менее, это отличная базовая модель и отправная точка для анализа.",
    "topic": "Машинное обучение"
  },
  {
    "id": 47,
    "question": "Прогнозирование.",
    "answer": "Прогнозирование (или предсказание) — это одна из ключевых задач анализа данных и машинного обучения. Цель — используя исторические данные и выявленные в них закономерности, предсказать будущие значения какой-либо величины. Примеры: прогноз продаж на следующий месяц, прогноз курса валюты, прогноз погоды, прогноз оттока клиентов (когда клиент уйдёт). Для прогнозирования используются методы временных рядов (ARIMA, экспоненциальное сглаживание) и методы машинного обучения (регрессия, нейронные сети). Важно понимать разницу между объяснительным моделированием (понять причины) и прогнозным моделированием (главное — точность предсказания, даже если модель — «чёрный ящик»). Ключевые этапы: сбор исторических данных, выбор горизонта прогнозирования (насколько далеко вперёд предсказываем), выбор модели, обучение, оценка точности на тестовых данных и собственно прогноз.",
    "topic": "Анализ данных"
  },
  {
    "id": 48,
    "question": "Ранжирование.",
    "answer": "Ранжирование — это задача упорядочивания объектов (документов, товаров, видео) по степени их релевантности или важности для конкретного запроса или пользователя. Это не классификация (где объект относится к классу) и не регрессия (где предсказывается число), а именно порядковая задача. Классический пример — поисковые системы (Google, Яндекс). Для поискового запроса система должна выдать список веб-страниц, отсортированный от самых полезных к самым бесполезным. Другой пример — рекомендательные системы, которые ранжируют товары для показа пользователю. Для решения задач ранжирования используются специальные алгоритмы (Learning to Rank — LTR), например, LambdaMART. Они обучаются на данных, где для запросов известен предпочтительный порядок документов (помеченных экспертами или по кликам пользователей). Метрики качества ранжирования — NDCG, MAP. Ранжирование лежит в основе персонализации контента в интернете.",
    "topic": "Машинное обучение"
  },
  {
    "id": 49,
    "question": "Интерполяция, экстраполяция, аппроксимация.",
    "answer": "Это три родственных метода работы с функциями и данными. 1) Интерполяция — нахождение промежуточных значений функции по известным точкам ВНУТРИ диапазона данных. Пример: известна температура в 9:00 (15°) и в 11:00 (19°). Интерполяция позволяет оценить температуру в 10:00 (скажем, 17°). Предполагается, что функция между точками ведёт себя плавно (линейно, полиномиально и т.д.). 2) Экстраполяция — предсказание значений функции ЗА ПРЕДЕЛАМИ известного диапазона данных. Пример: зная продажи за январь-ноябрь, попытаться предсказать продажи в декабре. Это рискованнее, чем интерполяция, так как закономерность может измениться. 3) Аппроксимация — подбор функции, которая приближённо описывает весь набор данных, не обязательно проходя точно через все точки. Цель — уловить общий тренд, сгладить шум. Например, проведение линии тренда на графике. Метод наименьших квадратов для линейной регрессии — это аппроксимация. Все три метода широко используются в анализе данных, прогнозировании и научных расчётах.",
    "topic": "Анализ данных"
  },
  {
    "id": 50,
    "question": "NOSQL.",
    "answer": "NoSQL (Not Only SQL) — это широкий класс систем управления базами данных, которые отличаются от традиционных реляционных (SQL) баз. Они созданы для решения проблем, с которыми SQL базы плохо справляются: огромные объёмы данных (Big Data), высокая скорость записи/чтения, гибкая схема данных, горизонтальное масштабирование (добавление новых серверов). Основные типы NoSQL баз: 1) Документоориентированные (MongoDB, Couchbase) — хранят данные в виде документов (например, JSON). У каждого документа может быть своя структура. 2) Ключ-значение (Redis, DynamoDB) — простейший тип, как словарь. Быстрые, хороши для кэширования. 3) Колоночные (Cassandra, HBase) — хранят данные по колонкам, а не по строкам. Эффективны для аналитических запросов по немногим полям. 4) Графовые (Neo4j) — хранят данные как узлы и связи между ними. Идеальны для социальных сетей, рекомендаций. NoSQL базы часто жертвуют ACID-свойствами (консистентностью) ради доступности и скорости (следуя теореме CAP).",
    "topic": "Big Data / Базы данных"
  },
  {
    "id": 51,
    "question": "Кластеризация (кластерный анализ).",
    "answer": "Кластеризация — это задача машинного обучения без учителя. Цель — разбить множество объектов на группы (кластеры) так, чтобы объекты внутри одной группы были похожи друг на друга, а объекты из разных групп — отличались. При этом заранее неизвестно, какие группы существуют и сколько их. Это процесс обнаружения естественной структуры в данных. Примеры: сегментация клиентов на группы со схожим поведением, группировка документов по темам, выделение аномалий (аномалии могут быть маленьким кластером). Популярные алгоритмы: k-средних (k-means), иерархическая кластеризация, DBSCAN. Выбор алгоритма зависит от формы кластеров (круглые, вытянутые), наличия шума и необходимости задавать число кластеров заранее. Кластеризация — мощный инструмент для разведочного анализа данных, когда нет размеченных ответов.",
    "topic": "Машинное обучение"
  },
  {
    "id": 52,
    "question": "Иерархическая кластеризация.",
    "answer": "Иерархическая кластеризация — это алгоритм, который строит не просто набор кластеров, а целое дерево вложенных разбиений (дендрограмму). Есть два основных подхода: 1) Агломеративная (восходящая) — начинается с того, что каждый объект считается отдельным кластером. Затем на каждом шаге два самых близких кластера объединяются в один. Процесс продолжается, пока все объекты не объединятся в один большой кластер. 2) Дивизитивная (нисходящая) — начинается с одного кластера, содержащего все объекты, и на каждом шаге кластер делится на два, пока каждый объект не станет отдельным кластером. Результат — дендрограмма, которая позволяет увидеть вложенность кластеров на разных уровнях детализации. Пользователь может «разрезать» дендрограмму на нужной высоте, чтобы получить желаемое число кластеров. Плюсы: наглядность, не требует заранее задавать число кластеров. Минусы: вычислительная сложность для больших данных, чувствительность к шуму.",
    "topic": "Машинное обучение"
  },
  {
    "id": 53,
    "question": "Алгоритм k-средних.",
    "answer": "K-means (k-средних) — это самый известный и простой алгоритм кластеризации. Его цель — разбить данные на K кластеров, где K задаётся пользователем. Алгоритм работает итеративно: 1) Инициализация: случайно выбираются K точек как начальные центры кластеров. 2) Шаг назначения: каждый объект назначается тому кластеру, центр которого к нему ближе всего (обычно по евклидову расстоянию). 3) Шаг обновления: для каждого кластера пересчитывается его центр (среднее арифметическое всех объектов кластера). 4) Шаги 2 и 3 повторяются до тех пор, пока назначения объектов перестанут меняться (или изменения будут незначительными). Алгоритм стремится минимизировать сумму квадратов расстояний от объектов до центров их кластеров. Плюсы: прост в реализации, быстрый, хорошо масштабируется. Минусы: нужно задавать K, чувствителен к начальному выбору центров (может сходиться к локальному минимуму), предполагает, что кластеры имеют сферическую форму и примерно одинаковый размер, чувствителен к выбросам.",
    "topic": "Машинное обучение"
  },
  {
    "id": 54,
    "question": "Признаковое описание объекта и матрица расстояний.",
    "answer": "Чтобы алгоритмы машинного обучения могли работать с объектами (клиентами, документами, изображениями), каждый объект нужно описать набором числовых или категориальных характеристик — признаков (features). Например, клиента можно описать признаками: возраст (число), пол (категория), средний чек (число). Это вектор признаков. Матрица расстояний (или матрица сходства) — это квадратная таблица, где на пересечении i-й строки и j-го столбца стоит расстояние (или мера сходства) между i-м и j-м объектом. Расстояние показывает, насколько объекты похожи: чем меньше расстояние, тем более похожи объекты. Для числовых признаков часто используют евклидово расстояние. Для категориальных — расстояние Хэмминга и другие. Матрица расстояний — основа для многих алгоритмов (кластеризации, k-NN). Её вычисление для N объектов требует O(N²) операций, что для больших данных становится проблемой.",
    "topic": "Машинное обучение"
  },
  {
    "id": 55,
    "question": "Гипотеза компактности.",
    "answer": "Гипотеза компактности — это основное допущение, лежащее в основе многих алгоритмов классификации и кластеризации. Она гласит: объекты, принадлежащие к одному классу (или кластеру), в пространстве признаков образуют компактные, хорошо разделимые области. То есть похожие объекты (с малым расстоянием между ними) с большой вероятностью относятся к одному классу, а объекты разных классов находятся далеко друг от друга. Это интуитивно понятно: все изображения кошек должны быть в одной области пространства признаков (похожи друг на друга), а изображения собак — в другой. Эта гипотеза позволяет таким методам, как k ближайших соседей (k-NN) и k-средних, работать. Однако она не всегда выполняется: классы могут быть размыты, пересекаться или иметь сложную форму. В таких случаях нужны более продвинутые методы (например, ядерные, которые преобразуют пространство признаков) или нейронные сети, которые сами учатся формировать компактные представления.",
    "topic": "Машинное обучение"
  },
  {
    "id": 56,
    "question": "Классификация.",
    "answer": "Классификация — одна из основных задач машинного обучения с учителем. Цель — построить модель (классификатор), которая по признаковому описанию объекта будет относить его к одному из заранее известных классов (категорий). Примеры: определить, является ли письмо спамом (классы: «спам»/«не спам»), диагностировать заболевание по симптомам (классы: «грипп», «ангина», «здоров»), распознать цифру на изображении (классы: 0,1,...,9). Процесс: 1) Есть обучающая выборка — множество объектов с известными классами. 2) Алгоритм обучения находит закономерности, связывающие признаки с классами. 3) Обученная модель применяется к новым объектам с неизвестным классом. Популярные алгоритмы классификации: логистическая регрессия, решающие деревья, метод опорных векторов (SVM), нейронные сети. Оценка качества классификации — точность, полнота, F-мера, ROC-кривая.",
    "topic": "Машинное обучение"
  },
  {
    "id": 57,
    "question": "Деревья принятия решений.",
    "answer": "Деревья принятия решений (Decision Trees) — это простой, но мощный алгоритм для задач классификации и регрессии, который строит модель в виде дерева. Дерево состоит из узлов и ветвей. Корневой узел содержит вопрос о значении одного из признаков (например, «Возраст > 30?»). В зависимости от ответа («Да»/«Нет») объект идёт по одной из ветвей к следующему узлу, где задаётся новый вопрос. Этот процесс продолжается, пока объект не дойдёт до листа — конечного узла, который содержит ответ (класс или числовое значение). Дерево обучается так: на каждом шаге выбирается признак и пороговое значение, которые лучше всего разделяют данные на «чистые» группы (чтобы в каждой группе были объекты преимущественно одного класса). Критерии выбора — прирост информации, индекс Джини. Плюсы: легко интерпретировать (можно проследить цепочку решений), не требует масштабирования признаков, работает с категориальными и числовыми данными. Минусы: склонны к переобучению, неустойчивы к небольшим изменениям данных.",
    "topic": "Машинное обучение"
  },
  {
    "id": 58,
    "question": "Случайный лес (random forest).",
    "answer": "Случайный лес — это ансамблевый алгоритм, который строит множество деревьев решений и объединяет их результаты для получения более точного и устойчивого предсказания. Он устраняет главный недостаток одиночного дерева — высокую дисперсию (переобучение). Как работает: 1) Используется бэггинг (bagging) — создаётся много (например, 100) bootstrap-выборок (выборок с повторением) из исходных данных. 2) Для каждой такой выборки строится своё дерево решений, но с важной модификацией: при выборе признака для разбиения узла рассматривается не все признаки, а только случайное подмножество (например, корень из общего числа). Это повышает разнообразие деревьев. 3) Для классификации итоговый ответ — это класс, выбранный большинством деревьев (голосование). Для регрессии — среднее предсказаний всех деревьев. Плюсы: высокая точность, устойчивость к переобучению и выбросам, может оценивать важность признаков, хорошо работает «из коробки». Минусы: менее интерпретируем, чем одно дерево; требует больше вычислительных ресурсов и времени.",
    "topic": "Машинное обучение"
  },
  {
    "id": 59,
    "question": "Сегментация.",
    "answer": "Сегментация в контексте анализа данных и маркетинга — это разделение совокупности объектов (клиентов, пользователей, товаров) на однородные группы (сегменты) по определённым критериям. Цель — лучше понимать аудиторию и принимать более точные решения для каждой группы. Например, сегментация клиентов по покупательскому поведению: «лояльные», «случайные», «ищущие скидки». После сегментации для каждого сегмента можно разработать свою стратегию: лояльным — программу привилегий, ищущим скидки — рассылки с акциями. Технически сегментация часто выполняется методами кластеризации (k-means, DBSCAN) или, если есть целевая переменная, деревьями решений. Также есть поведенческая, демографическая, географическая сегментация. Сегментация — ключевой инструмент в CRM, цифровом маркетинге и управлении продуктом.",
    "topic": "Анализ данных"
  },
  {
    "id": 60,
    "question": "Генетические алгоритмы.",
    "answer": "Генетические алгоритмы — это метод оптимизации и поиска решений, основанный на принципах естественного отбора и генетики (Дарвин). Они используются для решения сложных задач, где традиционные методы (перебор, градиентный спуск) неэффективны из-за большого пространства поиска. Как работают: 1) Инициализация популяции — создаётся множество случайных решений (хромосом), закодированных, например, строками битов. 2) Оценка приспособленности — каждая хромосома оценивается функцией приспособленности (fitness function), которая показывает, насколько решение хорошее. 3) Отбор — «лучшие» особи (решения) имеют больше шансов быть выбранными для «размножения». 4) Кроссовер (скрещивание) — пары выбранных особей обмениваются частями своих хромосом, порождая потомков. 5) Мутация — у потомков случайным образом изменяются некоторые гены (биты) для поддержания разнообразия. 6) Новая популяция заменяет старую, и процесс повторяется много поколений. Алгоритм постепенно «эволюционирует» к хорошим решениям. Применяется в проектировании, планировании, обучении нейросетей.",
    "topic": "Эволюционные алгоритмы"
  },
  {
    "id": 61,
    "question": "Требования для возможности применения генетического алгоритма.",
    "answer": "Чтобы генетический алгоритм (ГА) можно было эффективно использовать, задача должна обладать следующими свойствами: 1) Возможность кодирования решения в виде хромосомы (строки генов). Гены могут быть битами, числами, символами. Например, маршрут в задаче коммивояжёра можно закодировать как последовательность номеров городов. 2) Существование функции приспособленности (fitness function), которая для любой хромосомы может вычислить числовую оценку её «качества». Эта функция — главный двигатель эволюции. 3) Отсутствие необходимости в плавности или дифференцируемости. В отличие от градиентных методов, ГА не требует, чтобы функция была гладкой — она может быть разрывной, шумной, иметь много локальных минимумов. 4) Большое пространство поиска. ГА особенно полезны, когда полный перебор невозможен, а хорошие решения могут быть «собраны» из частей других решений (чтобы работал кроссовер). Если решение нельзя разбить на части или функция оценки очень медленная, ГА может быть неэффективен.",
    "topic": "Эволюционные алгоритмы"
  },
  {
    "id": 62,
    "question": "Детерминированные и стохастические системы.",
    "answer": "Детерминированная система — это система, в которой будущее состояние полностью и однозначно определяется её текущим состоянием и фиксированными правилами. Нет места случайности. Пример: движение планет по законам Ньютона, выполнение компьютерной программы на одних и тех же входных данных. Если знаешь начальные условия и уравнения, можно точно предсказать состояние в любой момент. Стохастическая система — это система, в которой присутствует элемент случайности. Будущее состояние определяется не только текущим состоянием и правилами, но и случайными факторами. Пример: броуновское движение частицы, трафик на дорогах, результат подбрасывания монеты. В таких системах можно говорить только о вероятностях тех или иных исходов. Большинство реальных систем (погода, биржевые котировки, поведение человека) — стохастические или детерминированно-хаотические. В машинном обучении стохастичность часто вводят намеренно (например, случайная инициализация весов, стохастический градиентный спуск) для избегания локальных минимумов и улучшения обобщения.",
    "topic": "Системы"
  },
  {
    "id": 63,
    "question": "Системы динамического хаоса.",
    "answer": "Динамический хаос — это явление, при котором поведение детерминированной системы (заданной точными уравнениями) выглядит случайным и непредсказуемым. Главный парадокс: система детерминирована (нет случайных внешних воздействий), но её долгосрочное поведение невозможно точно предсказать. Причина — экспоненциальная зависимость от начальных условий: даже ничтожное, незначимое изменение начального состояния через некоторое время приводит к кардинально разным результатам. Классическая метафора — «эффект бабочки»: взмах крыльев бабочки в Бразилии может вызвать торнадо в Техасе. Примеры хаотических систем: погода, турбулентность жидкостей, некоторые химические реакции, колебания маятника над несколькими магнитами. Хаотические системы часто имеют странные аттракторы. В анализе данных и машинном обучении понятие хаоса важно для понимания пределов предсказуемости сложных процессов.",
    "topic": "Системы"
  },
  {
    "id": 64,
    "question": "Аттрактор.",
    "answer": "Аттрактор — это множество состояний, к которому динамическая система эволюционирует с течением времени, из широкого набора начальных условий. Простыми словами, это «цель», к которой стремится система. Например, маятник с трением в конечном итоге остановится в самой нижней точке — это точка-аттрактор. Виды аттракторов: 1) Точка (устойчивое равновесие) — система приходит в одно состояние и остаётся в нём. 2) Предельный цикл — система выходит на периодические колебания (например, работа сердца). 3) Тор — более сложные квазипериодические движения. 4) Странный аттрактор — характерен для хаотических систем. Это сложная геометрическая структура (часто фрактальная), к которой система притягивается, но никогда не повторяет точно своё движение, оставаясь внутри этого множества. Аттракторы помогают описывать долгосрочное поведение сложных систем в физике, биологии, экономике.",
    "topic": "Системы"
  },
  {
    "id": 65,
    "question": "Рекурсивные и рекуррентные выражения.",
    "answer": "Эти термины часто путают. Рекурсивное выражение (или функция) — это такое выражение, которое определяет значение функции через её же значения для других (обычно меньших) аргументов. Классический пример — факториал: n! = n * (n-1)!, при этом задано базовое условие 0! = 1. Рекурсия широко используется в программировании и математике для разбиения задачи на подзадачи. Рекуррентное выражение (или рекуррентное соотношение) — это уравнение, которое определяет последовательность чисел, где каждый следующий член выражается через предыдущие. Например, числа Фибоначчи: F(n) = F(n-1) + F(n-2), с начальными условиями F(0)=0, F(1)=1. Рекуррентные соотношения часто используются для описания динамических процессов во времени. Ключевое различие: рекурсия — это общий принцип определения через себя, часто используемый в вычислениях, а рекуррентность — это конкретный тип математического уравнения для последовательностей, описывающий зависимость текущего состояния от предыдущих. В контексте ИИ рекуррентные нейронные сети (RNN) используют рекуррентные связи для обработки последовательностей.",
    "topic": "Математика / Основы"
  },
  {
    "id": 66,
    "question": "Фракталы.",
    "answer": "Фракталы — это геометрические объекты, которые обладают свойством самоподобия: их части выглядят как уменьшенные копии целого. Если увеличить маленький кусочек фрактала, вы увидите структуру, похожую на всю фигуру. Фракталы имеют дробную (не целую) размерность и часто очень сложную структуру, несмотря на то, что определяются простыми рекуррентными правилами. Классические примеры: множество Мандельброта, снежинка Коха, губка Менгера. Фракталы встречаются в природе: форма береговой линии, кроны деревьев, кровеносные сосуды, облака. В информатике и ИИ фракталы используются в сжатии изображений, компьютерной графике, моделировании природных ландшафтов. Также концепция фрактальности применяется в анализе временных рядов (фрактальный анализ) и в теории сложных систем для описания масштабно-инвариантных закономерностей.",
    "topic": "Математика"
  },
  {
    "id": 67,
    "question": "Фрактальная размерность.",
    "answer": "Фрактальная размерность — это мера, которая описывает, насколько сложный, «изрезанный» или заполняющий пространство является фрактальный объект. В отличие от привычной целочисленной размерности (точка — 0, линия — 1, плоскость — 2, куб — 3), фрактальная размерность может быть дробным числом. Например, береговая линия, будучи кривой (размерность 1), настолько извилиста, что «заполняет» пространство больше, чем простая линия, но меньше, чем плоскость. Её фрактальная размерность может быть около 1.2. Один из способов вычисления — метод подсчёта ячеек: накладываем на объект сетку с ячейками размера ε и считаем, сколько ячеек N(ε) пересекается с объектом. При уменьшении ε число пересечённых ячеек растёт по степенному закону: N(ε) ~ ε^(-D), где D — фрактальная размерность. Фрактальная размерность — важный показатель в анализе сложности сигналов, изображений и поведения динамических систем.",
    "topic": "Математика"
  },
  {
    "id": 68,
    "question": "Клеточные автоматы.",
    "answer": "Клеточный автомат — это дискретная модель, состоящая из сетки ячеек (клеток), каждая из которых может находиться в одном из конечного числа состояний (например, «живая» или «мёртвая»). Время также дискретно. Правила эволюции системы задаются локально: состояние каждой клетки на следующем шаге зависит только от её собственного состояния и состояний её соседей (например, 8 соседей в квадратной сетке). Все клетки обновляются одновременно по одним и тем же правилам. Самый известный пример — «Игра «Жизнь» Джона Конвея, где простые правила рождают невероятно сложные, динамические, похожие на жизнь паттерны. Клеточные автоматы используются для моделирования физических явлений (диффузия, поток жидкости), биологических процессов (рост колоний), в криптографии, и как вычислительные модели (тьюринг-полные). Они демонстрируют, как из простых локальных взаимодействий могут возникать глобальные сложные структуры.",
    "topic": "Моделирование"
  },
  {
    "id": 69,
    "question": "Одномерные бинарные клеточные автоматы.",
    "answer": "Это самый простой вид клеточных автоматов. Есть одномерная полоса клеток (строка), каждая клетка может быть в одном из двух состояний (0 или 1, «чёрная» или «белая»). Соседями клетки обычно считаются она сама и две соседние клетки (левый и правый сосед). Таким образом, для текущей тройки клеток (3 бита) существует 2^3 = 8 возможных конфигураций. Правило задаёт, в какое состояние (0 или 1) перейдёт центральная клетка на следующем шаге для каждой из этих 8 конфигураций. Всего таких правил может быть 2^8 = 256 (их нумеруют от 0 до 255, например, знаменитое «правило 110»). Начиная с начальной конфигурации (например, одна живая клетка посередине), система развивается, строя двумерную диаграмму, где по вертикали — время, по горизонтали — пространство. Несмотря на простоту, некоторые из этих правил порождают невероятно сложное, хаотическое или даже вычислительно универсальное поведение (как правило 110).",
    "topic": "Моделирование"
  },
  {
    "id": 70,
    "question": "Марковский процесс.",
    "answer": "Марковский процесс — это случайный процесс (последовательность случайных событий), обладающий свойством отсутствия памяти (марковским свойством). Это означает, что будущее состояние процесса зависит ТОЛЬКО от его текущего состояния и не зависит от того, как процесс пришёл в это состояние (то есть от предыстории). Прошлое «забывается». Пример: блуждание пьяницы. Где он окажется на следующем шаге, зависит только от того, где он стоит сейчас, а не от того, как он сюда пришёл. Марковские процессы бывают с дискретным временем (марковские цепи) и с непрерывным временем. Они широко применяются для моделирования систем в физике, химии, экономике, теории очередей, биоинформатике (например, моделирование эволюции последовательностей ДНК) и, конечно, в машинном обучении (скрытые марковские модели для распознавания речи, марковские процессы принятия решений в reinforcement learning).",
    "topic": "Теория вероятностей / Моделирование"
  },
  {
    "id": 71,
    "question": "Марковская цепь.",
    "answer": "Марковская цепь — это частный случай марковского процесса с дискретным множеством состояний и дискретным временем. Она полностью задаётся: 1) Множеством состояний S = {s1, s2, ...}. 2) Матрицей переходных вероятностей P, где элемент p_ij — вероятность перехода из состояния si в состояние sj за один шаг. Сумма вероятностей в каждой строке матрицы равна 1. 3) Начальным распределением вероятностей по состояниям. Эволюция цепи: в момент времени t система находится в некотором состоянии. В момент t+1 она случайным образом переходит в другое (или то же самое) состояние согласно вероятностям из матрицы P. Марковские цепи используются для моделирования случайных блужданий, прогнозирования (например, прогноз погоды, где состояния — «ясно», «дождь»), анализа текстов (цепочки слов), PageRank (веб-страницы как состояния, ссылки как переходы). Если цепь эргодична, то существует стационарное распределение — вероятности состояний, которые не меняются со временем.",
    "topic": "Теория вероятностей / Моделирование"
  },
  {
    "id": 72,
    "question": "Аксоны, дендриты, синапсы.",
    "answer": "Это основные части биологического нейрона, вдохновившие создание искусственных нейронных сетей. 1) Дендриты — это многочисленные ветвистые отростки нейрона, которые принимают сигналы от других нейронов. Они действуют как входные каналы. 2) Аксон — это длинный отросток нейрона, который передаёт выходной сигнал другим нейронам. Аксон обычно один, но может ветвиться на конце. 3) Синапсы — это специализированные соединения между аксоном одного нейрона и дендритом (или телом) другого. В синапсе происходит передача сигнала с помощью химических веществ (нейромедиаторов). Сила синаптической связи может меняться, что является основой обучения в мозге. В искусственной нейронной сети дендритам соответствуют взвешенные входы (произведение входного сигнала на вес связи), тело нейрона — сумматор и функция активации, аксон — выходной сигнал, а синапсам — веса связей, которые настраиваются в процессе обучения.",
    "topic": "Нейронные сети (биология)"
  },
  {
    "id": 73,
    "question": "Искусственный нейрон.",
    "answer": "Искусственный нейрон — это математическая модель, упрощённо имитирующая работу биологического нейрона. Это базовая вычислительная единица нейронной сети. Его работа состоит из трёх шагов: 1) Суммирование входов: нейрон получает несколько входных сигналов x1, x2, ..., xn (числа). Каждый вход умножается на соответствующий вес w1, w2, ..., wn (также числа), который показывает силу связи. Все взвешенные входы складываются, и к сумме часто добавляется смещение (bias) b. Получается линейная комбинация: z = w1*x1 + w2*x2 + ... + wn*xn + b. 2) Применение функции активации: сумма z пропускается через нелинейную функцию активации f. Это нужно, чтобы нейрон мог обучаться сложным нелинейным закономерностям. Результат: y = f(z). 3) Выход: полученное значение y передаётся на входы следующих нейронов или является выходом сети. Таким образом, искусственный нейрон — это просто устройство, которое вычисляет взвешенную сумму входов и применяет к ней нелинейную функцию.",
    "topic": "Нейронные сети"
  },
  {
    "id": 74,
    "question": "Функции активации.",
    "answer": "Функция активации определяет, насколько «возбуждён» будет искусственный нейрон на основе полученной взвешенной суммы входов. Она вводит нелинейность, без чего нейросеть была бы просто большой линейной моделью, неспособной обучаться сложным паттернам. Основные функции: 1) Сигмоида (логистическая): f(x) = 1/(1+e^{-x}). Сжимает число в диапазон (0,1). Раньше популярна, но сейчас реже из-за проблем с насыщением (градиенты близки к нулю при больших |x|). 2) Гиперболический тангенс (tanh): f(x) = (e^x - e^{-x})/(e^x + e^{-x}). Диапазон (-1, 1), центрирована вокруг нуля, что иногда удобнее. 3) ReLU (Rectified Linear Unit): f(x) = max(0, x). Самая популярная сегодня. Быстрая в вычислении, не насыщается при x>0, но «умирает» при x<0 (нейрон перестаёт обучаться). 4) Leaky ReLU: f(x) = max(αx, x), где α — маленькое число (например, 0.01). Пытается решить проблему «мёртвых» нейронов. 5) Softmax: используется в выходном слое для задач классификации, превращает числа в вероятности (их сумма равна 1). Выбор функции сильно влияет на скорость обучения и итоговую производительность сети.",
    "topic": "Нейронные сети"
  },
  {
    "id": 75,
    "question": "Дискретные / «аналоговые» нейроны.",
    "answer": "Эта классификация относится к типам выходных значений нейронов и моделям их работы. Дискретные (или бинарные) нейроны имеют выход, принимающий одно из двух значений (обычно 0/1 или -1/+1). Ранние модели нейронных сетей (перцептрон Розенблатта, нейрон МакКаллока-Питтса) были дискретными. Такие нейроны похожи на логические элементы. «Аналоговые» нейроны (чаще называют нейронами с непрерывной функцией активации) имеют выход, который может быть любым числом в некотором диапазоне (например, от 0 до 1 для сигмоиды, или любым неотрицательным числом для ReLU). Они более гибкие и позволяют использовать градиентные методы обучения (обратное распространение ошибки), так как функция активации дифференцируема. Почти все современные нейронные сети используют аналоговые нейроны. Термин «аналоговый» здесь условен и означает «непрерывный», в противоположность дискретному.",
    "topic": "Нейронные сети"
  },
  {
    "id": 76,
    "question": "Регулярные / нерегулярные ИНС.",
    "answer": "Регулярные нейронные сети — это сети, в которых связи между нейронами организованы по определённому, повторяющемуся шаблону (регулярной структуре). Например, полносвязные слои (каждый нейрон слоя N связан с каждым нейроном слоя N+1) или свёрточные слои (локальные связи, организованные в ядра). Нерегулярные нейронные сети — сети со случайной или произвольной топологией связей, не подчиняющейся простому шаблону. Они могут моделировать более сложные и специализированные структуры, напоминающие реальный мозг, где связи не столь однородны. Однако с нерегулярными сетями сложнее работать: трудно эффективно реализовать на GPU, сложно анализировать. Поэтому в практическом машинном learning почти всегда используются регулярные структуры (слои, свёртки), которые позволяют эффективно проводить параллельные вычисления и хорошо обобщать.",
    "topic": "Нейронные сети"
  },
  {
    "id": 77,
    "question": "Полносвязные / неполносвязные ИНС.",
    "answer": "Эта классификация описывает, как соединены нейроны между соседними слоями. В полносвязном слое (Fully Connected, Dense) каждый нейрон текущего слоя соединён со ВСЕМИ нейронами предыдущего слоя. Это наиболее общий и гибкий тип связи, но и самый «прожорливый» с точки зрения числа параметров (весов). Если в предыдущем слое 1000 нейронов, а в текущем 500, то весов будет 1000*500 = 500 000. Такие слои обычно используются в конце сети для классификации после извлечения признаков. В неполносвязных слоях нейроны соединены не со всеми, а только с некоторыми нейронами предыдущего слоя. Классический пример — свёрточный слой (Convolutional), где нейрон связан только с небольшой локальной областью (например, 3x3 пикселя) предыдущего слоя. Это резко уменьшает число параметров и позволяет сети эффективно работать с данными, имеющими пространственную структуру (изображения).",
    "topic": "Нейронные сети"
  },
  {
    "id": 78,
    "question": "ИНС без обратных связей / с обратными связями.",
    "answer": "Нейронные сети без обратных связей (Feedforward Neural Networks, FNN) — это сети, где сигнал распространяется строго в одном направлении: от входного слоя через скрытые слои к выходному. Информация не возвращается назад. Такие сети представляют собой направленный ациклический граф. Они используются для задач, где выход зависит только от текущего входного вектора, а не от предыдущих входов (например, классификация изображений). Нейронные сети с обратными связями (Recurrent Neural Networks, RNN) содержат циклы: выход нейрона может подаваться обратно на его же вход или на вход нейронов предыдущих слоев с задержкой. Это позволяет сети иметь «память» о предыдущих входных данных и обрабатывать последовательности (временные ряды, текст, речь). Однако обучение таких сетей сложнее из-за проблемы затухающих или взрывающихся градиентов. Более продвинутые архитектуры с обратными связями — LSTM и GRU.",
    "topic": "Нейронные сети"
  },
  {
    "id": 79,
    "question": "Слои ИНС.",
    "answer": "Слой в нейронной сети — это совокупность нейронов, которые получают входные сигналы из одного источника (например, предыдущего слоя или входных данных) и чьи выходы вычисляются одновременно. Слои позволяют организовать сеть иерархически. Основные типы слоёв: 1) Входной слой: не выполняет вычислений, просто представляет входные данные сети (например, пиксели изображения). Количество нейронов равно размерности входных данных. 2) Скрытые слои: слои между входным и выходным. Именно в них происходит извлечение и преобразование признаков. Может быть много скрытых слоёв (отсюда «глубокое» обучение). Типы скрытых слоёв: полносвязные (Dense), свёрточные (Convolutional), рекуррентные (Recurrent), пулинговые (Pooling) и др. 3) Выходной слой: выдаёт окончательный результат работы сети. Его структура зависит от задачи: один нейрон с сигмоидой для бинарной классификации, несколько нейронов с softmax для многоклассовой классификации, один нейрон (линейный) для регрессии. Комбинация разных типов слоёв образует архитектуру нейронной сети.",
    "topic": "Нейронные сети"
  },
  {
    "id": 80,
    "question": "Способы обучения ИНС.",
    "answer": "Обучение нейронной сети — это процесс настройки её весов (параметров) так, чтобы она решала поставленную задачу с минимальной ошибкой. Основные способы: 1) Обучение с учителем (Supervised Learning): самый распространённый метод. Есть размеченные данные — пары (вход, правильный ответ). Алгоритм, сравнивая выход сети с правильным ответом, вычисляет ошибку (функцию потерь) и с помощью метода обратного распространения ошибки (backpropagation) корректирует веса в сторону уменьшения этой ошибки (обычно градиентным спуском). 2) Обучение без учителя (Unsupervised Learning): данные не размечены. Сеть учится находить скрытые структуры в данных (кластеризация, уменьшение размерности). Пример — автоэнкодеры. 3) Обучение с подкреплением (Reinforcement Learning): сеть (агент) взаимодействует со средой, получая награды или штрафы за свои действия. Цель — максимизировать совокупную награду. Веса обновляются с помощью методов вроде Q-learning или Policy Gradient. 4) Трансферное обучение (Transfer Learning): использование предобученной на большой задаче сети и её дообучение на своей, меньшей задаче. Экономит время и данные.",
    "topic": "Нейронные сети"
  },
  {
    "id": 81,
    "question": "Переобучение и паралич сети.",
    "answer": "Переобучение (overfitting) — это ситуация, когда нейронная сеть слишком хорошо подстраивается под обучающие данные, включая их шум и случайные особенности, и в результате плохо работает на новых, ранее не виденных данных (не обобщает). Сеть «запоминает» обучающую выборку вместо того, чтобы выучить общие закономерности. Признаки: ошибка на обучающих данных очень мала, а на тестовых — велика. Методы борьбы: увеличение размера выборки, регуляризация (L1, L2), dropout, ранняя остановка. Паралич сети (точнее, проблема насыщения или «затухания градиентов») возникает, особенно в глубоких сетях с сигмоидой или tanh, когда выходы нейронов попадают в области, где производная функции активации близка к нулю (например, на «полках» сигмоиды). При обратном распространении градиент ошибки умножается на эти маленькие производные и становится исчезающе малым. В результате веса в начальных слоях практически не обновляются, и обучение останавливается. Решение: использование функций активации, устойчивых к этой проблеме (ReLU), специальные методы инициализации весов, остаточные связи (ResNet).",
    "topic": "Нейронные сети"
  },
  {
    "id": 82,
    "question": "Плюсы и минусы нейронных сетей.",
    "answer": "Плюсы нейронных сетей: 1) Высокая точность: на многих задачах (распознавание изображений, речи, NLP) показывают наилучшие результаты. 2) Универсальность: способны аппроксимировать любую сложную функцию (теорема о универсальной аппроксимации). 3) Автоматическое извлечение признаков: не требуют ручного инжиниринга признаков, учатся сами из сырых данных. 4) Устойчивость к шуму и искажениям. 5) Способность обучаться на огромных массивах данных. Минусы: 1) «Чёрный ящик»: сложно интерпретировать, как и почему сеть приняла то или иное решение. 2) Требовательность к данным: для обучения нужны большие размеченные датасеты. 3) Требовательность к вычислительным ресурсам: обучение глубоких сетей требует мощных GPU и времени. 4) Склонность к переобучению: требует тщательной настройки и применения методов регуляризации. 5) Неопределённость в выборе архитектуры: часто нужно перебирать множество вариантов. 6) Проблемы с обучением: затухающие/взрывающиеся градиенты, локальные минимумы.",
    "topic": "Нейронные сети"
  },
  {
    "id": 83,
    "question": "Глубокие нейронные сети.",
    "answer": "Глубокие нейронные сети (Deep Neural Networks, DNN) — это нейронные сети, имеющие много скрытых слоёв (обычно более 3). «Глубина» позволяет сети выстраивать иерархию признаков: первые слои учатся распознавать простые паттерны (края, текстуры в изображениях), средние — комбинируют их в более сложные (части объектов), а последние — в целые объекты или сцены. Именно глубина стала ключом к прорывам в компьютерном зрении, обработке речи и естественного языка в 2010-х годах. Глубокие сети требуют: 1) Больших данных для обучения. 2) Мощного железа (GPU/TPU). 3) Специальных приёмов для обучения (правильная инициализация, функции активации ReLU, методы регуляризации типа dropout, skip-connections в ResNet). Архитектуры глубоких сетей: глубокие полносвязные сети, свёрточные нейронные сети (CNN) для изображений, рекуррентные (RNN, LSTM) для последовательностей, трансформеры для текста. Глубокое обучение — это синоним современного успешного машинного обучения.",
    "topic": "Нейронные сети"
  },
  {
    "id": 84,
    "question": "Свёрточные нейронные сети.",
    "answer": "Свёрточные нейронные сети (Convolutional Neural Networks, CNN) — это специальный тип нейронных сетей, разработанный для эффективной работы с данными, имеющими сеточную структуру (изображения, звуковые спектрограммы). Их ключевые особенности: 1) Свёрточные слои: нейроны в таком слое применяют операцию свёртки — небольшой фильтр (ядро) «скользит» по входному изображению, вычисляя скалярные произведения. Это позволяет обнаруживать локальные признаки (края, текстуры) независимо от их положения. 2) Общие веса (weight sharing): один и тот же фильтр применяется ко всему изображению, что резко уменьшает число параметров по сравнению с полносвязным слоем. 3) Пулинговые слои (подвыборка): уменьшают размерность карт признаков, объединяя соседние нейроны (например, беря максимум — max pooling). Это даёт инвариантность к небольшим сдвигам и уменьшает объём вычислений. CNN обычно состоят из чередующихся блоков свёртки+пулинга, а в конце — несколько полносвязных слоёв для классификации. Они стали стандартом для задач компьютерного зрения (распознавание, детекция, сегментация).",
    "topic": "Нейронные сети"
  },
  {
    "id": 85,
    "question": "Рекуррентные нейронные сети.",
    "answer": "Рекуррентные нейронные сети (Recurrent Neural Networks, RNN) — это класс нейронных сетей, предназначенных для обработки последовательностей данных (временные ряды, текст, речь). Их отличительная черта — наличие циклов (обратных связей), которые позволяют информации сохраняться и передаваться от одного шага последовательности к другому. У RNN есть «скрытое состояние», которое обновляется на каждом шаге и содержит информацию о предыдущих элементах последовательности. Это позволяет сети иметь «память». Однако у простых RNN есть проблема затухающих градиентов — они плохо запоминают долгосрочные зависимости. Для решения этой проблемы были созданы более сложные архитектуры: 1) Долгая краткосрочная память (LSTM) — имеет три «ворота» (gate), которые управляют потоком информации: что забыть, что запомнить, что вывести. 2) Управляемые рекуррентные блоки (GRU) — упрощённая версия LSTM. RNN и их варианты используются для машинного перевода, генерации текста, анализа тональности, распознавания речи. Сейчас их во многих задачах NLP вытесняют трансформеры, но для некоторых задач с последовательностями RNN всё ещё актуальны.",
    "topic": "Нейронные сети"
  },
  {
    "id": 86,
    "question": "Какая главная функция искусственного нейрона.",
    "answer": "Главная функция искусственного нейрона — это вычисление взвешенной суммы своих входных сигналов, добавление смещения (bias) и применение нелинейной функции активации к этому результату. Формально: нейрон получает входы x1, x2, ..., xn, умножает каждый на соответствующий вес w1, w2, ..., wn, суммирует все произведения, добавляет смещение b, а затем пропускает полученное число z = Σ(w_i * x_i) + b через функцию активации f, производя выход y = f(z). В двух словах: нейрон — это «взвешенное суммирование + нелинейное преобразование». Именно нелинейная функция активации позволяет нейросети быть универсальным аппроксиматором и решать сложные задачи. Без неё вся сеть, независимо от количества слоёв, сводилась бы к простой линейной модели.",
    "topic": "Нейронные сети"
  },
  {
    "id": 87,
    "question": "Чем самообучение отличается от обучения.",
    "answer": "В контексте ИИ и машинного обучения: Обучение (в широком смысле) — это процесс, при котором система (алгоритм) улучшает свою производительность на основе опыта (данных). Обычно подразумевается наличие внешнего источника информации: при обучении с учителем — это размеченные данные (правильные ответы), при обучении с подкреплением — награды от среды. Самообучение (self-learning) — это более узкий и специфичный термин. Часто он означает, что система способна обучаться без явного внешнего учителя, генерируя для себя обучающие данные или цели. Примеры: 1) Алгоритмы кластеризации — находят структуры в данных без заданных меток. 2) Автоэнкодеры — учатся сжимать и восстанавливать данные, а цель (восстановленный вход) создаётся из самих входных данных. 3) Самообучение в reinforcement learning — агент может генерировать для себя опыт, исследуя среду. В обиходе «самообучение» иногда путают с «обучением без учителя» (unsupervised learning), но последнее — более чёткий академический термин.",
    "topic": "Основные понятия"
  },
  {
    "id": 88,
    "question": "Что лежит в основе имитационного подхода построения систем искусственного интеллекта.",
    "answer": "В основе имитационного подхода лежит идея копирования (имитации) принципов работы естественного интеллекта — человеческого мозга или биологических систем. Вместо того чтобы создавать ИИ «с нуля» на основе логических правил, мы пытаемся воспроизвести те механизмы, которые, как мы знаем, работают в природе. Конкретно это означает: 1) Использование искусственных нейронных сетей, вдохновлённых структурой и работой биологических нейронов. 2) Применение эволюционных алгоритмов, имитирующих естественный отбор. 3) Моделирование поведения роя (муравьиные алгоритмы) или стаи. Ключевое предположение: если природа создала интеллект таким путём, то, воспроизводя его основные принципы, мы тоже сможем создать интеллект. Этот подход контрастирует с символьным подходом (логика, правила) и привёл к самым большим успехам в современном ИИ, особенно в области машинного обучения и глубоких нейронных сетей.",
    "topic": "Методология"
  },
  {
    "id": 89,
    "question": "Как определяется текущее состояние нейрона.",
    "answer": "Текущее состояние искусственного нейрона определяется его выходным значением (активацией) в данный момент времени. Это выходное значение вычисляется на основе текущих входных сигналов (которые пришли от других нейронов или из внешней среды), текущих весов связей и смещения, а также функции активации. Формально: состояние нейрона — это значение y = f(Σ(w_i * x_i) + b), где x_i — текущие входы, w_i — текущие веса, b — смещение, f — функция активации. В рекуррентных нейронных сетях у нейрона есть также внутреннее скрытое состояние (hidden state), которое сохраняется между шагами и используется при обработке следующего элемента последовательности. Таким образом, состояние нейрона — это не просто число, а результат всей его текущей «настройки» (весов) и полученных им сигналов.",
    "topic": "Нейронные сети"
  },
  {
    "id": 90,
    "question": "Сколько этапов проходит экспертная система при разработке.",
    "answer": "Разработка экспертной системы (ЭС) — это итеративный процесс, который обычно включает в себя следующие основные этапы: 1) Идентификация проблемы: определение целей, задач, пользователей и экономической целесообразности. 2) Извлечение знаний: самый сложный и длительный этап. Инженер по знаниям (knowledge engineer) работает с экспертами-людьми, чтобы выявить, формализовать и структурировать их знания, опыт и правила принятия решений. 3) Проектирование: выбор инструментов и архитектуры ЭС (на основе правил, фреймов и т.д.), разработка структуры базы знаний и механизма вывода. 4) Реализация (кодирование): создание прототипа ЭС на выбранной платформе или языке. 5) Тестирование и оценка: проверка системы на реальных задачах, оценка её точности, полноты и полезности экспертами и пользователями. 6) Внедрение и интеграция: установка системы в рабочую среду, обучение пользователей. 7) Сопровождение и развитие: постоянное обновление базы знаний, исправление ошибок, добавление новых правил по мере накопления опыта. Эти этапы часто перекрываются и повторяются по спирали.",
    "topic": "Экспертные системы"
  },
  {
    "id": 91,
    "question": "Что такое решающее дерево в машинном обучении и какие задачи оно позволяет решать?",
    "answer": "Решающее дерево (Decision Tree) — это модель машинного обучения, которая представляет собой древовидную структуру, где каждый внутренний узел соответствует проверке значения одного из признаков, каждая ветвь — результату этой проверки, а каждый лист — значению целевой переменной (классу или числу). Дерево строит иерархию простых правил «если-то», что делает его очень интерпретируемым. Оно позволяет решать две основные задачи: 1) Классификация: когда целевая переменная — категория (например, спам/не спам). Такие деревья называются деревьями классификации, и в листьях у них метки классов. 2) Регрессия: когда целевая переменная — числовая (например, цена дома). Такие деревья называются деревьями регрессии, и в листьях у них конкретные числа или модели (например, среднее значение целевой переменной для объектов, попавших в этот лист). Решающие деревья также используются для анализа важности признаков и как базовые алгоритмы в ансамблевых методах (Random Forest, Gradient Boosting).",
    "topic": "Машинное обучение (деревья)"
  },
  {
    "id": 92,
    "question": "Опишите принцип построения решающего дерева. Какие этапы включает алгоритм обучения?",
    "answer": "Принцип построения решающего дерева — рекурсивное разбиение пространства признаков на всё более однородные (чистые) области. Алгоритм обучения (например, CART, ID3, C4.5) обычно работает так: 1) Начинаем с корневого узла, содержащего всю обучающую выборку. 2) Выбираем наилучший признак и пороговое значение для разбиения данных в текущем узле. «Наилучший» означает разбиение, которое максимизирует однородность получившихся подгрупп. Для классификации используют критерии (метрики) типа прироста информации (Information Gain) или индекса Джини. Для регрессии — минимизацию дисперсии или MSE. 3) Создаём два (или более) дочерних узла, соответствующих результатам разбиения, и отправляем в них соответствующие подмножества данных. 4) Рекурсивно повторяем шаги 2-3 для каждого дочернего узла, используя только те данные, которые в него попали. 5) Останавливаем рекурсию, когда выполняется одно из условий останова: все объекты в узле принадлежат одному классу (или имеют малую дисперсию), достигнута максимальная глубина дерева, число объектов в узле меньше минимального порога, или разбиение не даёт значимого улучшения. Тогда узел объявляется листом, и ему присваивается прогноз: для классификации — самый частый класс в узле, для регрессии — среднее значение целевой переменной.",
    "topic": "Машинное обучение (деревья)"
  },
  {
    "id": 93,
    "question": "Что такое признак, узел, ветвь и лист в контексте решающего дерева?",
    "answer": "В контексте решающего дерева: 1) Признак (feature, атрибут) — это характеристика объекта, по значению которой в узле дерева происходит проверка (разбиение). Например, «возраст > 30», «цвет = красный». Дерево выбирает, какие признаки использовать и в каком порядке. 2) Узел (node) — это элемент дерева, который представляет либо проверку признака (внутренний узел), либо конечный прогноз (листовой узел). Внутренний узел содержит условие (например, «зарплата ≤ 50 000?»). 3) Ветвь (branch, edge) — это стрелка, выходящая из узла, которая соответствует одному из возможных результатов проверки. Например, из узла с условием «возраст > 30» выходят две ветви: «Да» (если условие истинно) и «Нет» (если ложно). Ветви ведут к дочерним узлам. 4) Лист (leaf, terminal node) — это конечный узел дерева, который не имеет дочерних узлов. Он содержит итоговый прогноз модели: для классификации — метку класса (например, «выдать кредит»), для регрессии — числовое значение. Путь от корня до листа представляет собой последовательность правил, ведущих к конкретному решению.",
    "topic": "Машинное обучение (деревья)"
  },
  {
    "id": 94,
    "question": "Какие критерии разбиения используются при построении решающих деревьев для задач классификации?",
    "answer": "Для выбора наилучшего признака и порога разбиения в узле дерева классификации используются критерии, измеряющие «примесь» (impurity) узла. Цель — уменьшить примесь после разбиения. Основные критерии: 1) Индекс Джини (Gini impurity): измеряет вероятность неправильной классификации случайно выбранного объекта из узла, если бы его метку предсказали согласно распределению классов в узле. Формула: Gini = 1 - Σ(p_i²), где p_i — доля объектов i-го класса в узле. Чем меньше Джини (ближе к 0), тем однороднее узел (все объекты одного класса). Алгоритм CART использует индекс Джини. 2) Энтропия (Entropy) и Прирост информации (Information Gain): Энтропия измеряет степень неопределенности в узле: H = - Σ(p_i * log₂(p_i)). Прирост информации — это разность энтропии родительского узла и взвешенной суммы энтропий дочерних узлов. Алгоритм выбирает разбиение с максимальным приростом информации (максимально уменьшающим неопределённость). Алгоритмы ID3 и C4.5 используют этот подход. Оба критерия дают похожие результаты, но Джини немного быстрее вычисляется.",
    "topic": "Машинное обучение (деревья)"
  },
  {
    "id": 95,
    "question": "В чём заключается энтропия и как она используется при выборе оптимального разбиения?",
    "answer": "Энтропия в теории информации — это мера неопределённости или хаоса. В контексте решающих деревьев энтропия узла показывает, насколько «смешаны» классы в этом узле. Если в узле объекты только одного класса — энтропия минимальна (равна 0), неопределённости нет. Если классы представлены равномерно — энтропия максимальна. Формула: H = - Σ(p_i * log₂(p_i)), где p_i — доля объектов i-го класса в узле. Как используется: при выборе разбиения алгоритм вычисляет энтропию родительского узла (до разбиения) и энтропии дочерних узлов (после разбиения). Затем вычисляется взвешенная сумма энтропий дочерних узлов (вес — доля объектов, попавших в каждый дочерний узел). Разность между энтропией родителя и этой взвешенной суммой называется приростом информации (Information Gain). Алгоритм перебирает возможные разбиения по всем признакам и порогам и выбирает то разбиение, которое даёт максимальный прирост информации, то есть максимально уменьшает неопределённость (энтропию). Таким образом, энтропия — это ориентир, который ведёт дерево к созданию чистых, однородных листьев.",
    "topic": "Машинное обучение (деревья)"
  },
  {
    "id": 96,
    "question": "Что такое прирост информации (information gain) и почему он важен при обучении дерева?",
    "answer": "Прирост информации (Information Gain, IG) — это величина, показывающая, насколько уменьшается неопределённость (энтропия) в системе после того, как мы разобьём данные по определённому признаку. Формально: IG = Энтропия(родительский_узел) - Σ ( (|дочерний_узел| / |родительский_узел|) * Энтропия(дочерний_узел) ). Суммирование ведётся по всем дочерним узлам, образованным разбиением. Почему он важен: При обучении решающего дерева на каждом шаге нужно выбрать самое лучшее разбиение. Прирост информации — это численная мера «полезности» разбиения. Чем больше IG, тем больше неопределённости было устранено, тем более чистыми (однородными по классам) стали подгруппы. Алгоритм перебирает все возможные признаки и пороги, вычисляет для каждого разбиения IG, и выбирает то, которое даёт максимальный прирост. Это жадная стратегия: на каждом шаге делается локально оптимальный выбор, который в итоге (но не всегда глобально) приводит к построению эффективного дерева. IG является ключевым понятием в алгоритмах ID3 и C4.5.",
    "topic": "Машинное обучение (деревья)"
  },
  {
    "id": 97,
    "question": "Объясните отличие деревьев классификации от деревьев регрессии.",
    "answer": "Основное отличие — тип целевой переменной и, как следствие, прогноз в листьях и критерий разбиения. Деревья классификации используются, когда нужно предсказать категорию (класс). Пример: определить, болен ли пациент (классы: «да», «нет»). В листьях такого дерева хранится метка класса (обычно самый частый класс среди объектов листа). Критерии для выбора разбиения — это меры примеси классов (индекс Джини, энтропия), цель — создать листья с максимально однородным составом по классам. Деревья регрессии используются, когда нужно предсказать числовое значение. Пример: предсказать стоимость дома. В листьях такого дерева хранится числовое значение — обычно среднее значение целевой переменной для объектов, попавших в этот лист. Критерий для выбора разбиения — минимизация дисперсии (или среднеквадратичной ошибки, MSE) внутри образовавшихся подгрупп. Цель — чтобы значения внутри каждого листа были как можно ближе к среднему (малая дисперсия). В остальном принцип построения (рекурсивное разбиение) одинаков.",
    "topic": "Машинное обучение (деревья)"
  },
  {
    "id": 98,
    "question": "Что такое переобучение решающего дерева и почему деревья особенно склонны к этой проблеме?",
    "answer": "Переобучение решающего дерева — это ситуация, когда дерево становится слишком сложным, глубоким и детализированным, идеально подстраиваясь под обучающие данные, включая шум и выбросы. В результате дерево теряет способность обобщать и плохо работает на новых данных. Признаки: дерево имеет много узлов и большую глубину, ошибка на обучающей выборке почти нулевая, а на тестовой — высокая. Почему деревья склонны к переобучению: 1) Жадный алгоритм построения: дерево может продолжать разбивать узлы до тех пор, пока в каждом листе не окажется по одному объекту, добиваясь 100% точности на обучающих данных. 2) Высокая гибкость (высокая дисперсия): дерево может создать чрезвычайно сложные, зазубренные границы решений, которые соответствуют шуму. 3) Отсутствие индуктивного смещения: по сравнению, например, с линейными моделями, у деревьев мало врождённых ограничений на форму зависимости. Поэтому, если не контролировать сложность, дерево «выучит» обучающую выборку наизусть. Борьба с переобучением: ограничение максимальной глубины, минимального числа объектов в листе, обрезка (pruning), использование ансамблей (Random Forest).",
    "topic": "Машинное обучение (деревья)"
  },
  {
    "id": 99,
    "question": "Какие существуют методы ограничения роста дерева (предобрезка и постобрезка)?",
    "answer": "Чтобы избежать переобучения, рост дерева нужно ограничивать. Есть две основные стратегии: предобрезка (pre-pruning) и постобрезка (post-pruning). Предобрезка (досрочная остановка): процесс построения дерева останавливается до того, как оно станет идеально подогнанным под обучающие данные. Критерии остановки (гиперпараметры): 1) Максимальная глубина дерева. 2) Минимальное число объектов в узле для разбиения (например, если в узле меньше 10 объектов, не разбиваем его дальше). 3) Минимальное число объектов в листе. 4) Минимальное улучшение критерия (например, прирост информации меньше порога). Предобрезка проще и быстрее, но есть риск недообучения — остановиться слишком рано и не выучить важные закономерности. Постобрезка (послеобрезка): сначала строится большое, переобученное дерево (часто до листьев с одним объектом), а затем оно «подрезается» — некоторые поддеревья заменяются листьями. Алгоритм (например, Cost-Complexity Pruning в CART) рассматривает каждое поддерево и решает, стоит ли его заменить листом, исходя из компромисса между точностью на обучающих данных и сложностью дерева. Постобрезка часто даёт лучшее качество, чем предобрезка, но требует больше вычислений.",
    "topic": "Машинное обучение (деревья)"
  },
  {
    "id": 100,
    "question": "В чём заключается идея Random Forest и какие недостатки одиночного дерева он устраняет?",
    "answer": "Идея Random Forest (Случайного леса) в том, чтобы построить множество решающих деревьев и объединить их предсказания, чтобы получить более точный и устойчивый результат. Это ансамблевый метод, который использует два ключевых приёма: 1) Бэггинг (Bootstrap Aggregating) — каждое дерево обучается на своей собственной случайной подвыборке из исходных данных (выборка с возвращением). Это создаёт разнообразие в данных для каждого дерева. 2) Случайный выбор признаков — при построении каждого узла каждого дерева рассматривается не весь набор признаков, а только случайное небольшое подмножество (например, квадратный корень из общего числа). Это заставляет деревья быть ещё более разными. Итоговый прогноз: для классификации — голосование большинством, для регрессии — усреднение. Random Forest устраняет главные недостатки одиночного дерева: 1) Высокую дисперсию (склонность к переобучению) — усреднение предсказаний многих деревьев сглаживает шум. 2) Неустойчивость — маленькое изменение данных может сильно изменить структуру одного дерева, но на ансамбль это почти не повлияет. 3) Часто даёт более высокую точность. При этом Random Forest сохраняет некоторые преимущества деревьев, например, может оценивать важность признаков.",
    "topic": "Машинное обучение (деревья)"
  },
  {
    "id": 101,
    "question": "Объясните принцип работы бэггинга (bagging) и его связь с ансамблями решающих деревьев.",
    "answer": "Бэггинг (Bootstrap Aggregating) — это общий метод построения ансамблей моделей, который уменьшает дисперсию предсказаний и помогает бороться с переобучением. Принцип: 1) Генерируется множество (например, 100) bootstrap-выборок из исходного набора данных. Bootstrap-выборка — это случайная выборка того же размера, что и исходные данные, но с возвращением (один объект может попасть в выборку несколько раз, а какие-то — ни разу). 2) На каждой такой выборке независимо обучается одна и та же модель (например, решающее дерево). 3) Итоговый прогноз ансамбля получается путём агрегации (усреднения для регрессии, голосования для классификации) предсказаний всех обученных моделей. Связь с деревьями: решающие деревья — это модели с высокой дисперсией (сильно меняются при небольшом изменении данных). Бэггинг идеально подходит для таких «неустойчивых» моделей. Объединяя множество деревьев, обученных на разных выборках, мы получаем более плавную и устойчивую решающую функцию. Random Forest — это развитие идеи бэггинга для деревьев, добавляющее также случайность в выборе признаков для ещё большего разнообразия моделей в ансамбле.",
    "topic": "Машинное обучение (деревья)"
  },
  {
    "id": 102,
    "question": "В чём отличие градиентного бустинга от случайного леса?",
    "answer": "И Random Forest (RF), и Gradient Boosting (GB) — это ансамблевые методы на основе деревьев, но их философия и алгоритм обучения кардинально различаются. Random Forest использует параллельное обучение (деревья строятся независимо друг от друга) и технику бэггинга (выборки с возвращением). Он создаёт множество «сильных» и сложных деревьев (часто их не ограничивают по глубине), а затем усредняет их, чтобы уменьшить дисперсию. Градиентный бустинг обучает деревья последовательно (адаптивно). Каждое новое дерево учится не на исходных данных, а на ошибках (остатках) предыдущих деревьев. Каждое дерево в бустинге обычно небольшое (глубина 3-6), оно играет роль «слабого ученика», который исправляет недочёты текущей модели. В итоге бустинг строит одну сильную модель как сумму многих слабых. Основные отличия: 1) Обучение: RF — параллельное, GB — последовательное. 2) Деревья: RF использует глубокие деревья, GB — мелкие. 3) Цель: RF снижает дисперсию, GB снижает смещение (bias). 4) Чувствительность: GB более чувствителен к настройкам и шуму, но часто даёт более высокую точность, если правильно настроен. RF более устойчив «из коробки».",
    "topic": "Машинное обучение (деревья)"
  },
  {
    "id": 103,
    "question": "Почему ансамбли на основе деревьев (Random Forest, Gradient Boosting) часто показывают высокое качество на табличных данных?",
    "answer": "Табличные данные — это структурированные данные, где объекты описаны набором признаков (столбцов) разных типов (числовые, категориальные). Ансамбли деревьев особенно хороши для таких данных по нескольким причинам: 1) Деревья не требуют масштабирования или сложной предобработки признаков. Они одинаково хорошо работают с признаками разных масштабов и смесями числовых и категориальных типов. 2) Деревья автоматически отбирают важные признаки и игнорируют неважные (через выбор разбиений). 3) Они хорошо улавливают нелинейные зависимости и взаимодействия между признаками (например, если важна комбинация «возраст > 30 И зарплата < 50000»). 4) Ансамбли (RF, GB) усиливают сильные стороны деревьев и компенсируют их слабости (переобучение, неустойчивость). Random Forest за счёт усреднения даёт устойчивую и точную модель, хорошо работающую «из коробки». Gradient Boosting, тщательно исправляя ошибки, часто достигает максимальной точности. Напротив, нейронные сети, которые блестяще работают с неструктурированными данными (изображения, текст), на табличных данных могут требовать больше усилий по настройке и не всегда превосходят деревья.",
    "topic": "Машинное обучение (деревья)"
  },
  {
    "id": 104,
    "question": "Какие преимущества и недостатки решающих деревьев по сравнению с линейными моделями?",
    "answer": "Преимущества решающих деревьев перед линейными моделями: 1) Интерпретируемость: дерево можно визуализировать и понять логику принятия решений, что важно в медицине, финансах. Линейная модель даёт только коэффициенты. 2) Нелинейность: деревья автоматически улавливают сложные нелинейные зависимости и взаимодействия признаков. Линейная модель предполагает линейную связь, которую нужно вручную задавать через полиномиальные признаки. 3) Работа с разными типами данных: деревья не требуют масштабирования признаков и могут работать с категориальными признаками без one-hot кодирования. Линейные модели часто требуют тщательной предобработки. 4) Устойчивость к выбросам: деревья менее чувствительны к экстремальным значениям. Недостатки деревьев по сравнению с линейными моделями: 1) Склонность к переобучению: деревья легко создают слишком сложные модели. Линейные модели с регуляризацией более устойчивы. 2) Неустойчивость: небольшое изменение данных может привести к построению совершенно другого дерева. Линейная модель меняется не так drastically. 3) Низкая экстраполяционная способность: деревья плохо предсказывают за пределами диапазона обучающих данных (дают константу). Линейная модель может экстраполировать тренд. 4) В целом, на небольших линейно разделимых наборах данных линейные модели могут быть проще и эффективнее.",
    "topic": "Машинное обучение (деревья)"
  },
  {
    "id": 105,
    "question": "Приведите примеры практических задач, где решающие деревья и их ансамбли применяются наиболее эффективно.",
    "answer": "Решающие деревья и их ансамбли (Random Forest, Gradient Boosting) — это рабочие лошадки прикладного машинного обучения. Они особенно эффективны в следующих задачах: 1) Кредитный скоринг: оценка вероятности дефолта заёмщика на основе его данных (доход, возраст, кредитная история). Требуется интерпретируемость и высокая точность. 2) Медицинская диагностика: классификация заболеваний по симптомам и результатам анализов. Деревья помогают выявлять ключевые признаки. 3) Отток клиентов (churn prediction): предсказание, уйдёт ли клиент от оператора связи или банка. Ансамбли хорошо работают с табличными данными о клиентских транзакциях. 4) Рекомендательные системы (часть): для ранжирования или фильтрации контента на основе признаков пользователя и товара. 5) Задачи с большим количеством категориальных признаков: например, прогнозирование отклика на маркетинговую кампанию по демографическим данным. 6) Задачи, где важна оценка значимости признаков: например, в биоинформатике для выявления генов, связанных с болезнью. 7) Задачи соревнований по машинному обучению (Kaggle): библиотеки XGBoost, LightGBM и CatBoost (реализации градиентного бустинга) долгое время были безусловными лидерами на табличных данных.",
    "topic": "Машинное обучение (деревья)"
  }
]